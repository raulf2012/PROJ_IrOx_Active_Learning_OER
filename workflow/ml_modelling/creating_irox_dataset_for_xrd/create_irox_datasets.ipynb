{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# New ML Active Learning Workflow\n",
    "---\n",
    "\n",
    "32 (Too big, not computed),\n",
    "\n",
    "226 (Not computed),"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#| - OUT_OF_SIGHT\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ase.visualize import view\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path, unique_ids_path,\n",
    "    df_features_pre_opt_path,\n",
    "    df_features_post_opt_path,\n",
    "    ids_to_discard__too_many_atoms_path,\n",
    "    )\n",
    "\n",
    "from plotting.my_plotly import my_plotly_plot\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "sys.path.insert(0,\n",
    "    os.path.join(os.environ[\"PROJ_irox\"], \"workflow/ml_modelling\"))\n",
    "\n",
    "from ml_methods import create_mixed_df\n",
    "\n",
    "from ase_modules.ase_methods import view_in_vesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# stoich_i = \"AB3\"\n",
    "stoich_i = \"AB2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "#     df_bulk_dft = pickle.load(fle)\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Duplicate Analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling\"))\n",
    "from ml_methods import get_data_for_al\n",
    "from ccf_similarity.ccf import CCF\n",
    "\n",
    "out_dict = get_data_for_al(\n",
    "    stoich=stoich_i, verbose=False,\n",
    "    drop_too_many_atoms=True)\n",
    "\n",
    "df_dij = out_dict[\"df_dij\"]\n",
    "df_bulk_dft = out_dict[\"df_bulk_dft\"]\n",
    "\n",
    "CCF_i = CCF(df_dij=df_dij, d_thresh=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_bulk_dft = df_bulk_dft[df_bulk_dft.source == \"raul\"]\n",
    "\n",
    "ids_to_drop = []\n",
    "for id_i in df_bulk_dft.index.tolist():\n",
    "    simil_dict_i = CCF_i.i_all_similar(id_i)\n",
    "\n",
    "    # for key, val in simil_dict_i.items():\n",
    "    #     tmp = 42\n",
    "\n",
    "    if simil_dict_i is not None:\n",
    "        similar_ids = [id_i] + list(simil_dict_i.keys())\n",
    "\n",
    "        df_i = df_bulk_dft.loc[similar_ids]\n",
    "\n",
    "        ids_to_drop_i = df_i.sort_values(\"energy_pa\").iloc[1:].index.tolist()\n",
    "\n",
    "        ids_to_drop.extend(ids_to_drop_i)\n",
    "\n",
    "        \n",
    "ids_to_drop__duplicates = ids_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# df_dij.loc[[\n",
    "# #     \"zimixdvdxd\",\n",
    "#     \"6fcdbh9fz2\",\n",
    "#     ]]\n",
    "\n",
    "# df_dij.loc[\"6fcdbh9fz2\"]\n",
    "\n",
    "# df_tmp = df_dij\n",
    "# \"6fcdbh9fz2\" in df_tmp.index.tolist()\n",
    "\n",
    "# \"6fcdbh9fz2\" in df_bulk_dft.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Filtering dataframes to the correct stoicheometry"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TEMP DROP DUPLICATE and OUTLIER SYSTEMS"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# path_i = os.path.join(\n",
    "#     os.environ[\"PROJ_irox\"],\n",
    "#     \"workflow/ml_modelling/00_ml_workflow/191102_new_workflow/00_abx_al_runs/out_data\",\n",
    "#     \"duplicates.pickle\")\n",
    "# with open(path_i, \"rb\") as fle:\n",
    "#     duplicates_dict = pickle.load(fle)\n",
    "#     ids_to_drop__duplicates = duplicates_dict[stoich_i]\n",
    "\n",
    "# #############################################################################\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/visualizing_data/out_data\",\n",
    "    \"outlier_features.json\")\n",
    "with open(path_i, 'r') as f:\n",
    "    ids_to_drop__outliers = json.load(f)\n",
    "\n",
    "with open(ids_to_discard__too_many_atoms_path, \"rb\") as fle:\n",
    "    ids_to_drop__too_many_atoms = pickle.load(fle)\n",
    "\n",
    "# #############################################################################\n",
    "ids_to_drop = [] + \\\n",
    "    ids_to_drop__duplicates + \\\n",
    "    []\n",
    "    # ids_to_drop__too_many_atoms + \\\n",
    "    # ids_to_drop__outliers + \\\n",
    "\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))\n",
    "ids_to_drop = list(set(ids_to_drop))\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Filter ids ##################################################################\n",
    "df_ids = df_ids[\n",
    "    (df_ids[\"stoich\"] == stoich_i) & \\\n",
    "    (df_ids[\"source\"] != \"oqmd\") & \\\n",
    "    (df_ids[\"source\"] != \"raul_oer\") & \\\n",
    "    [True for i in range(len(df_ids))]\n",
    "    ]\n",
    "\n",
    "print(\"df_ids.shape:\", df_ids.shape)\n",
    "# IDS TO DROP\n",
    "df_ids = df_ids[~df_ids[\"unique_ids\"].isin(ids_to_drop)]\n",
    "unique_ids = df_ids[\"unique_ids\"].tolist()\n",
    "\n",
    "index_filter = np.intersect1d(df_bulk_dft.index, unique_ids)\n",
    "df_bulk_dft = df_bulk_dft.loc[index_filter]\n",
    "df_bulk_dft = df_bulk_dft[df_bulk_dft[\"source\"] != \"chris\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "directory = \"out_data/\" + stoich_i + \"_structures_all\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "directory = \"out_data/\" + stoich_i + \"_structures\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"atoms\",\n",
    "    \"form_e_chris\",\n",
    "    \"id\",\n",
    "    \"path\",\n",
    "    \"source\",\n",
    "    \"stoich\",\n",
    "    ]\n",
    "\n",
    "df_select = df_bulk_dft.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_bulk_dft = df_bulk_dft.sort_values(\"energy_pa\")\n",
    "\n",
    "df_bulk_dft[\"energy_order_id\"] = [i for i in range(len(df_bulk_dft))]\n",
    "\n",
    "df_select = df_bulk_dft.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import ase\n",
    "\n",
    "ase.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stoich_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_select.to_csv(\"out_data/data_table_\" + stoich_i + \".csv\")\n",
    "\n",
    "for i_cnt, row_i in df_bulk_dft.iterrows():\n",
    "    atoms = row_i[\"atoms\"]\n",
    "    file_name_i = \"\" + \\\n",
    "        str(row_i[\"energy_order_id\"]).zfill(3) + \\\n",
    "        \"__\" + \\\n",
    "        \"id-unique\" + \\\n",
    "        \"_\" + \\\n",
    "        row_i.name + \\\n",
    "        \"__\" + \\\n",
    "        \"id-short\" + \\\n",
    "        \"_\" + \\\n",
    "        str(row_i[\"id_old\"]).zfill(3) + \\\n",
    "        \".cif\"\n",
    "    \n",
    "    # atoms.write(\"out_data/\" + stoich_i + \"_structures_all/\" + file_name_i)\n",
    "    atoms.write(\"out_data/\" + stoich_i + \"_structures/\" + file_name_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# df_bulk_dft.loc[\"cubqbpzd7k\"]\n",
    "\n",
    "# df_bulk_dft.sort_values(\"energy_pa\").iloc[0:3].index\n",
    "df_bulk_dft.sort_values(\"energy_pa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# df_dij.loc[\n",
    "#     [\"xw9y6rbkxr\", \"8p8evt9pcg\"],\n",
    "#     [\"xw9y6rbkxr\", \"8p8evt9pcg\"],\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_bulk_dft[df_bulk_dft[\"stoich\"] == \"AB3\"].sort_values(\"energy_pa\")\n",
    "\n",
    "# -6.469847\n",
    "# -6.467450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# -6.469847 - -6.46745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
