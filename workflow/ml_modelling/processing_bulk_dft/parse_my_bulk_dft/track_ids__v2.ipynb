{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# #############################################################################\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path, unique_ids_path,\n",
    "    static_irox_structures_path)\n",
    "\n",
    "# #############################################################################\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from out_data.inputs_nersc_iro2 import ignore_ids as ignore_ids_nersc_iro2\n",
    "# from out_data.inputs_nersc_iro3 import ignore_ids as ignore_ids_nersc_iro3\n",
    "ignore_ids_nersc_iro3 = []\n",
    "\n",
    "from out_data.inputs_sher_iro2 import ignore_ids as ignore_ids_sher_iro2\n",
    "from out_data.inputs_sher_iro3 import ignore_ids as ignore_ids_sher_iro3\n",
    "\n",
    "from out_data.inputs_slac_iro2 import ignore_ids as ignore_ids_slac_iro2\n",
    "# from out_data.inputs_slac_iro3 import ignore_ids as ignore_ids_slac_iro3\n",
    "ignore_ids_slac_iro3 = []\n",
    "\n",
    "ignore_ids_iro2 = ignore_ids_nersc_iro2 + ignore_ids_sher_iro2 + ignore_ids_slac_iro2\n",
    "ignore_ids_iro3 = ignore_ids_nersc_iro3 + ignore_ids_sher_iro3 + ignore_ids_slac_iro3\n",
    "\n",
    "print(\"len(ignore_ids_iro2):\", len(ignore_ids_iro2))\n",
    "print(\"len(ignore_ids_iro3):\", len(ignore_ids_iro3))\n",
    "\n",
    "ignore_ids_dict = {\n",
    "    \"AB2\": ignore_ids_iro2,\n",
    "    \"AB3\": ignore_ids_iro3}\n",
    "\n",
    "\n",
    "# Pickling data ######################################################\n",
    "import os; import pickle\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"ignore_ids.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(ignore_ids_dict, fle)\n",
    "# #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from out_data.ids_to_run_nersc_iro2 import ids_to_run as ids_to_run_nersc_iro2\n",
    "from out_data.ids_to_run_nersc_iro3 import ids_to_run as ids_to_run_nersc_iro3\n",
    "\n",
    "from out_data.ids_to_run_slac_iro2 import ids_to_run as ids_to_run_slac_iro2\n",
    "# from out_data.ids_to_run_slac_iro3 import ids_to_run as ids_to_run_slac_iro3\n",
    "ids_to_run_slac_iro3 = []\n",
    "\n",
    "from out_data.ids_to_run_sher_iro2 import ids_to_run as ids_to_run_sher_iro2\n",
    "from out_data.ids_to_run_sher_iro3 import ids_to_run as ids_to_run_sher_iro3\n",
    "\n",
    "ids_to_run_iro2 = ids_to_run_nersc_iro2 + ids_to_run_sher_iro2 + ids_to_run_slac_iro2\n",
    "ids_to_run_iro3 = ids_to_run_nersc_iro3 + ids_to_run_sher_iro3 + ids_to_run_slac_iro3\n",
    "\n",
    "ids_to_run_iro3 = list(set(ids_to_run_iro3))\n",
    "ids_to_run_iro2 = list(set(ids_to_run_iro2))\n",
    "\n",
    "ids_to_run_sher = ids_to_run_sher_iro2 + ids_to_run_sher_iro3\n",
    "ids_to_run_nersc = ids_to_run_nersc_iro2 + ids_to_run_nersc_iro3\n",
    "ids_to_run_slac = ids_to_run_slac_iro2 + ids_to_run_slac_iro3\n",
    "\n",
    "ids_to_run_sher = list(set(ids_to_run_sher))\n",
    "ids_to_run_nersc = list(set(ids_to_run_nersc))\n",
    "ids_to_run_slac = list(set(ids_to_run_slac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoich_i = \"AB2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "    df_bulk_dft = pickle.load(fle)\n",
    "    df_bulk_dft = df_bulk_dft[(df_bulk_dft[\"source\"] == \"raul\")]\n",
    "\n",
    "with open(static_irox_structures_path, \"rb\") as fle:\n",
    "    df_static = pickle.load(fle)\n",
    "    df_static = df_static[(df_static[\"source\"] == \"chris\")]\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)\n",
    "\n",
    "with open(\"out_data/df_new_jobs.pickle\", \"rb\") as fle:\n",
    "    df_new_jobs = pickle.load(fle)\n",
    "\n",
    "with open(\"out_data/df_irox_long.pickle\", \"rb\") as fle:\n",
    "    df_irox_long = pickle.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static[df_static[\"id_old\"] == 182]\n",
    "# df_static\n",
    "\n",
    "df_bulk_dft[df_bulk_dft[\"id_old\"] == 182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_irox_long[df_irox_long[\"id\"] == \"250\"].sort_values(\"revision\")\n",
    "\n",
    "df_irox_long[\n",
    "    (df_irox_long[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_irox_long[\"id\"] == \"250\")\n",
    "    ].sort_values(\"revision\")\n",
    "\n",
    "df_new_jobs[\n",
    "    (df_new_jobs[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_new_jobs[\"id\"] == \"250\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids_in_process = df_new_jobs[df_new_jobs[\"stoich\"] == stoich_i][\"id\"]\n",
    "unique_ids_in_process = unique_ids_in_process.unique().tolist()\n",
    "unique_ids_in_process = [i for i in unique_ids_in_process if i.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    atoms = row_i[\"atoms\"]\n",
    "    num_atoms = atoms.get_number_of_atoms()\n",
    "    return(num_atoms)\n",
    "\n",
    "df_static[\"num_atoms\"] = df_static.apply(method, axis=1)\n",
    "df_static = df_static.drop([\"atoms\", \"path\", \"source\",], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_i =df_static.iloc[0]\n",
    "\n",
    "id_old = str(row_i[\"id_old\"]).zfill(3)\n",
    "\n",
    "if id_old in unique_ids_in_process:\n",
    "    tmp = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bulk_dft[\n",
    "    (df_bulk_dft[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_bulk_dft[\"source\"] == \"raul\")\n",
    "    # () & \\\n",
    "    ].index.unique()\n",
    "\n",
    "#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    new_column_values_dict = {}\n",
    "\n",
    "    stoich_i = row_i[\"stoich\"]\n",
    "\n",
    "    id_old = row_i[\"id_old\"]\n",
    "    id_old_str = str(id_old).zfill(3)\n",
    "\n",
    "    # #########################################################################\n",
    "    if row_i.name in df_bulk_dft.index:\n",
    "        new_column_values_dict[\"done\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"done\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    if id_old_str in unique_ids_in_process:\n",
    "        new_column_values_dict[\"being_processed\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"being_processed\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    if stoich_i == \"AB2\":\n",
    "        ignore_ids_i = ignore_ids_iro2\n",
    "    elif stoich_i == \"AB3\":\n",
    "        ignore_ids_i = ignore_ids_iro3\n",
    "    # id_old = str(row_i[\"id_old\"]).zfill(3)\n",
    "\n",
    "    if id_old_str in ignore_ids_i:\n",
    "        new_column_values_dict[\"ignored\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"ignored\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    # stoich_i = row_i[\"stoich\"] == \"AB2\"\n",
    "    if stoich_i == \"AB2\":\n",
    "        ids_to_run_i = ids_to_run_iro2\n",
    "    elif stoich_i == \"AB3\":\n",
    "        ids_to_run_i = ids_to_run_iro3\n",
    "\n",
    "    if id_old in ids_to_run_i:\n",
    "        new_column_values_dict[\"ids_to_run\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"ids_to_run\"] = False\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    if stoich_i == \"AB2\":\n",
    "        if id_old in ids_to_run_sher_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"sherlock\"\n",
    "        elif id_old in ids_to_run_nersc_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"nersc\"\n",
    "        elif id_old in ids_to_run_slac_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"slac\"\n",
    "        else:\n",
    "            new_column_values_dict[\"cluster\"] = np.nan\n",
    "\n",
    "    elif stoich_i == \"AB3\":\n",
    "        if id_old in ids_to_run_sher_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"sherlock\"\n",
    "        elif id_old in ids_to_run_nersc_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"nersc\"\n",
    "        elif id_old in ids_to_run_slac_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"slac\"\n",
    "        else:\n",
    "            new_column_values_dict[\"cluster\"] = np.nan\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    df_new_jobs_i = df_new_jobs[\n",
    "        (df_new_jobs[\"stoich\"] == stoich_i) & \\\n",
    "        (df_new_jobs[\"id\"] == str(id_old).zfill(3))\n",
    "        ]\n",
    "\n",
    "    if len(df_new_jobs_i) == 1:\n",
    "        row_j = df_new_jobs_i.iloc[0]\n",
    "\n",
    "        action_j = row_j[\"action\"]\n",
    "\n",
    "        bool_0 = action_j == 'Job is busy, will skip'\n",
    "        bool_1 = action_j == 'Time out or failed | Restarting isif 3 calc'\n",
    "        bool_2 = action_j == 'Time out or failed | Restarting isif 7 calc'\n",
    "        bool_3 = action_j == 'Job done | ISIF 3 done | --> isif 2'\n",
    "        \n",
    "        if bool_0 or bool_1 or bool_2 or bool_3:\n",
    "            new_column_values_dict[\"running\"] = True\n",
    "        else:\n",
    "            new_column_values_dict[\"running\"] = False\n",
    "\n",
    "        bool_0 = action_j == \"Error, need manual attention\"\n",
    "        bool_1 = action_j == \"Couldn't figure out what to do\"\n",
    "        if bool_0 or bool_1:\n",
    "            new_column_values_dict[\"error\"] = True\n",
    "\n",
    "    elif len(df_new_jobs_i) == 0:\n",
    "        new_column_values_dict[\"running\"] = np.nan\n",
    "    elif len(df_new_jobs_i) > 1:\n",
    "        # print(df_new_jobs_i)\n",
    "        # display(df_new_jobs_i)\n",
    "        new_column_values_dict[\"running\"] = \"TEMP, more than 1 sys\"\n",
    "\n",
    "    # #########################################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "\n",
    "df_i = df_static\n",
    "df_static = df_i.apply(method, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "697 total AB2  structures in df_static\n",
    "\n",
    "------------------------------------\n",
    "87 systems with num_atoms >= 100\n",
    "\n",
    "132 systems with num_atoms >= 75"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "697 - \\\n",
    "(\n",
    "    132 + \\\n",
    "    469 + \n",
    "    0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "565 - (469 + 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bulk_dft_iro2_ids_list = df_bulk_dft[\n",
    "    (df_bulk_dft[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_bulk_dft[\"source\"] == \"raul\")\n",
    "    # () & \\\n",
    "    ].index.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df = df_static.drop(\"static_id\", axis=1)\n",
    "# df = df[\n",
    "#     (df[\"running\"] == \"TEMP, more than 1 sys\") & \\\n",
    "# #     (df[\"stoich\"] == stoich_i) & \\\n",
    "# #     (df[\"num_atoms\"] < 75) & \\\n",
    "# #     (df[\"num_atoms\"] > 75) & \\\n",
    "# #     (df[\"num_atoms\"] < 100) & \\\n",
    "# #     (df[\"being_processed\"] == False) & \\\n",
    "# #     (df[\"done\"] == False) & \\\n",
    "# #     (df[\"ignored\"] == False) & \\\n",
    "# #     (df[\"running\"] == True) & \\\n",
    "#     [True for i in range(len(df))]\n",
    "#     ]\n",
    "# print(\"df.shape:\", df.index.unique().shape)\n",
    "# # display(df)\n",
    "\n",
    "# ids_in_2_clusters = df[\"id_old\"].tolist()\n",
    "\n",
    "# ids_in_2_clusters\n",
    "\n",
    "\n",
    "# for id_i in ids_in_2_clusters:\n",
    "#     tmp = 42\n",
    "\n",
    "#     id_i = str(id_i).zfill(3)\n",
    "\n",
    "#     print(\n",
    "#         \"id_i:\", id_i,\n",
    "#         \"|\",\n",
    "#         df_irox_long[df_irox_long[\"id\"] == id_i][\"source\"].unique().tolist()    \n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "697 # Total AB2\n",
    "565 # AB2 | < 75 atoms\n",
    "105 # AB2 | < 75 atoms | Not  done\n",
    "# 32 # AB2 | < 75 atoms | Not  done | Not ignored\n",
    "18 # AB2 | < 75 atoms | Not  done | Not ignored\n",
    "\n",
    "\n",
    "# #####################################\n",
    "14  # Currently running\n",
    "7  # That have identified error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_static.drop(\"static_id\", axis=1)\n",
    "df = df[\n",
    "    (df[\"stoich\"] == stoich_i) & \\\n",
    "    (df[\"num_atoms\"] < 75) & \\\n",
    "#     (df[\"num_atoms\"] > 75) & \\\n",
    "#     (df[\"num_atoms\"] < 100) & \\\n",
    "#     (df[\"being_processed\"] == False) & \\\n",
    "    (df[\"done\"] == False) & \\\n",
    "    (df[\"ignored\"] == False) & \\\n",
    "#     (df[\"running\"] == True) & \\\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "print(\"df.shape:\", df.index.unique().shape)\n",
    "# display(df)\n",
    "\n",
    "\n",
    "\n",
    "from running_ids import sherlock_ids_running, slac_ids_running\n",
    "\n",
    "\n",
    "slac_ids_running\n",
    "\n",
    "print(len([i for i in df[\"id_old\"].tolist() if str(i).zfill(3) in sherlock_ids_running]))\n",
    "print(len([i for i in df[\"id_old\"].tolist() if str(i).zfill(3) in slac_ids_running]))\n",
    "\n",
    "# sherlock_ids_running\n",
    "# df[\"id_old\"].tolist()\n",
    "\n",
    "np.sort(\n",
    "[i for i in df[\"id_old\"].tolist() if str(i).zfill(3) not in sherlock_ids_running + slac_ids_running]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sherlock_ids_running + slac_ids_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60,\n",
    "# 89,\n",
    "# 118,\n",
    "155,\n",
    "193,\n",
    "# 236,\n",
    "# 250,\n",
    "275,\n",
    "303,\n",
    "# 483,\n",
    "# 649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_irox_long.head()\n",
    "\n",
    "# idf_irox_long[df_irox_long[\"id\"] == \"155\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static.head()\n",
    "df_static[df_static[\"id_old\"] == 155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tmp = [i for i in df[\"id_old\"].tolist() if str(i).zfill(3) not in list(sherlock_ids_running + slac_ids_running)]\n",
    "\n",
    "len(list_tmp)\n",
    "\n",
    "list_tmp\n",
    "\n",
    "df_new_jobs[\n",
    "    (df_new_jobs[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_new_jobs[\"id\"].isin([str(i).zfill(3) for i in list_tmp]))\n",
    "    # (df_new_jobs[\"id\"].isin([\"250\", \"275\"]))\n",
    "    # (df_new_jobs[\"id\"] == \"250\")\n",
    "    ].sort_values(\"id\")\n",
    "\n",
    "\n",
    "# df_new_jobs[df_new_jobs[\"id\"] == \"250\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static.head()\n",
    "\n",
    "# df_static[df_static[\"id_old\"] == \"182\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(i).zfill(3) for i in list_tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_static.head()\n",
    "\n",
    "# df_static[df_static[\"id_old\"] == 250]\n",
    "\n",
    "# [i for i in df.index.unique().tolist() if i not  in df_bulk_dft_iro2_ids_list]\n",
    "\n",
    "# df_bulk_dft.loc['vwxrnun48g']\n",
    "\n",
    "# 'vwxrnun48g' in df_bulk_dft_iro2_ids_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ab2_sher = df_irox_long[df_irox_long[\"source\"] == \"sherlock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_master_list = []\n",
    "\n",
    "df_ab2_i = df_irox_long[\n",
    "    (df_irox_long[\"source\"] == \"sherlock\") & \\\n",
    "    (df_irox_long[\"stoich\"] == \"AB2\")\n",
    "    ]\n",
    "\n",
    "df_ab2_i_complete = df_ab2_i[\n",
    "    (df_ab2_i[\"isif\"] == 2.) & \\\n",
    "    (df_ab2_i[\"completed\"] == True)\n",
    "    ]\n",
    "print(\n",
    "    df_ab2_i_complete[\"id\"].shape,\n",
    "    df_ab2_i_complete[\"id\"].unique().shape,\n",
    "    )\n",
    "ids_i = df_ab2_i_complete[\"id\"].unique().tolist()\n",
    "ids_master_list.extend(ids_i)\n",
    "\n",
    "print(\"sherlock:\", df_ab2_i_complete.shape)\n",
    "\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "df_ab2_i = df_irox_long[\n",
    "    (df_irox_long[\"source\"] == \"slac\") & \\\n",
    "    (df_irox_long[\"stoich\"] == \"AB2\")\n",
    "    ]\n",
    "\n",
    "df_ab2_i_complete = df_ab2_i[\n",
    "    (df_ab2_i[\"isif\"] == 2.) & \\\n",
    "    (df_ab2_i[\"completed\"] == True)\n",
    "    ]\n",
    "print(\n",
    "    df_ab2_i_complete[\"id\"].shape,\n",
    "    df_ab2_i_complete[\"id\"].unique().shape,\n",
    "    )\n",
    "ids_i = df_ab2_i_complete[\"id\"].unique().tolist()\n",
    "ids_master_list.extend(ids_i)\n",
    "\n",
    "print(\"slac:\", df_ab2_i_complete.shape)\n",
    "\n",
    "# #############################################################################\n",
    "df_ab2_i = df_irox_long[\n",
    "    (df_irox_long[\"source\"] == \"nersc\") & \\\n",
    "    (df_irox_long[\"stoich\"] == \"AB2\")\n",
    "    ]\n",
    "\n",
    "df_ab2_i_complete = df_ab2_i[\n",
    "    (df_ab2_i[\"isif\"] == 2.) & \\\n",
    "    (df_ab2_i[\"completed\"] == True)\n",
    "    ]\n",
    "print(\n",
    "    df_ab2_i_complete[\"id\"].shape,\n",
    "    df_ab2_i_complete[\"id\"].unique().shape,\n",
    "    )\n",
    "ids_i = df_ab2_i_complete[\"id\"].unique().tolist()\n",
    "ids_master_list.extend(ids_i)\n",
    "\n",
    "print(\"nersc:\", df_ab2_i_complete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_ids_list = []\n",
    "for i in ids_master_list:\n",
    "    # print(i)\n",
    "    if ids_master_list.count(i) > 1:\n",
    "#         print(i)\n",
    "        repeated_ids_list.append(i)\n",
    "\n",
    "\n",
    "set(repeated_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_irox_long.head()\n",
    "\n",
    "# df_tmp = df_irox_long[\n",
    "#     (df_irox_long[\"id\"] == \"303\") & \\\n",
    "#     (df_irox_long[\"stoich\"] == \"AB2\")\n",
    "#     ].sort_values([\"source\", \"revision\"])\n",
    "\n",
    "# print(df_tmp.loc[745][\"atoms\"].get_potential_energy())\n",
    "# print(df_tmp.loc[1475][\"atoms\"].get_potential_energy())\n",
    "\n",
    "# df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_master_list)\n",
    "\n",
    "len(set(ids_master_list))\n",
    "\n",
    "ids_master_list[0:4]\n",
    "\n",
    "ids_master_list = [int(i) for i in ids_master_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bulk_dft[df_bulk_dft[\"id_old\"] == 338]\n",
    "\n",
    "# df_bulk_dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list_from_main_script = df_bulk_dft[\n",
    "    (df_bulk_dft[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_bulk_dft[\"source\"] == \"raul\")\n",
    "    ][\"id_old\"].tolist()\n",
    "\n",
    "\n",
    "ids_not_in_main_list = [i for i in ids_master_list if i not in ids_list_from_main_script]\n",
    "\n",
    "\n",
    "ids_not_in_main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle; import os\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/processing_bulk_dft/parse_my_bulk_dft/out_data\",\n",
    "    \"parse_data.pickle\"\n",
    "    )\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    job_dirs, completed_ids = pickle.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# print(len(job_dirs))\n",
    "# print(len(completed_ids))\n",
    "\n",
    "# # [i for i in ids_0 if i not in job_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_static[df_static[\"num_atoms\"] >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # (464,)\n",
    "\n",
    "# unique_ids_in_process = df_new_jobs[df_new_jobs[\"stoich\"] == \"AB2\"][\"id\"]\n",
    "# unique_ids_in_process = unique_ids_in_process.unique().tolist()\n",
    "\n",
    "# [i for i in unique_ids_in_process if i.isdigit()]\n",
    "\n",
    "# # unique_ids_in_process[0].isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# all_ab2_ids = df_static[\n",
    "#     (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "\n",
    "# ab2_completed_ids = df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "# #     (df_static[\"ignored\"] == True) & \\\n",
    "# #     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "# #     (df_static[\"ids_to_run\"] == False) & \\\n",
    "# #     (df_static[\"running\"] == True) & \\\n",
    "# #     (df_static[\"error\"] == True) & \\\n",
    "#     (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "# ab2_errored_ids = df_static[\n",
    "# #     (df_static[\"done\"] == True) & \\\n",
    "# #     (df_static[\"ignored\"] == True) & \\\n",
    "# #     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "# #     (df_static[\"ids_to_run\"] == False) & \\\n",
    "# #     (df_static[\"running\"] == True) & \\\n",
    "#     (df_static[\"error\"] == True) & \\\n",
    "#     (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "\n",
    "# ab2_large_atoms_ids = df_static[\n",
    "# #     (df_static[\"done\"] == True) & \\\n",
    "# #     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "# #     (df_static[\"ids_to_run\"] == False) & \\\n",
    "# #     (df_static[\"running\"] == True) & \\\n",
    "# #     (df_static[\"error\"] == True) & \\\n",
    "#     (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "\n",
    "# ab2_ids_accounted_for = ab2_completed_ids + ab2_errored_ids + ab2_large_atoms_ids\n",
    "\n",
    "\n",
    "# ab2_ids_not_acc = [i for i in all_ab2_ids if i not in ab2_ids_accounted_for]\n",
    "# # [i for i all_ab3_ids]\n",
    "\n",
    "# # df_new_jobs\n",
    "\n",
    "# df_static.loc[ab2_ids_not_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# all_ab3_ids = df_static[\n",
    "#     (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "\n",
    "# ab3_completed_ids = df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "# #     (df_static[\"ignored\"] == True) & \\\n",
    "# #     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "# #     (df_static[\"ids_to_run\"] == False) & \\\n",
    "# #     (df_static[\"running\"] == True) & \\\n",
    "# #     (df_static[\"error\"] == True) & \\\n",
    "#     (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "# ab3_errored_ids = df_static[\n",
    "# #     (df_static[\"done\"] == True) & \\\n",
    "# #     (df_static[\"ignored\"] == True) & \\\n",
    "# #     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "# #     (df_static[\"ids_to_run\"] == False) & \\\n",
    "# #     (df_static[\"running\"] == True) & \\\n",
    "#     (df_static[\"error\"] == True) & \\\n",
    "#     (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "\n",
    "# ab3_large_atoms_ids = df_static[\n",
    "# #     (df_static[\"done\"] == True) & \\\n",
    "# #     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "# #     (df_static[\"ids_to_run\"] == False) & \\\n",
    "# #     (df_static[\"running\"] == True) & \\\n",
    "# #     (df_static[\"error\"] == True) & \\\n",
    "#     (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "#     [True for i in range(len(df_static))]\n",
    "#     ].index.tolist()\n",
    "\n",
    "\n",
    "# ab3_ids_accounted_for = ab3_completed_ids + ab3_errored_ids + ab3_large_atoms_ids\n",
    "\n",
    "\n",
    "# ab3_ids_not_acc = [i for i in all_ab3_ids if i not in ab3_ids_accounted_for]\n",
    "# # [i for i all_ab3_ids]\n",
    "\n",
    "# # df_new_jobs\n",
    "\n",
    "# df_static.loc[ab3_ids_not_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_new_jobs[\"action\"].unique().tolist()\n",
    "\n",
    "# [\n",
    "#  'ALL DONE! | ISIF 2',\n",
    "#  'Ignoring this id',\n",
    "#  'Job is busy, will skip',\n",
    "#  'Error, need manual attention',\n",
    "#  \"Couldn't figure out what to do\",\n",
    "#  'Time out or failed | Restarting isif 3 calc',\n",
    "#  'Time out or failed | Restarting isif 7 calc',\n",
    "#  'Job done | ISIF 3 done | --> isif 2',\n",
    "# ]\n",
    "\n",
    "# [\n",
    "#  'ALL DONE! | ISIF 2',\n",
    "#  'Ignoring this id',\n",
    "#  'Job is busy, will skip',\n",
    "#  'Error, need manual attention',\n",
    "#  \"Couldn't figure out what to do\",\n",
    "#  'Time out or failed | Restarting isif 3 calc',\n",
    "#  'Time out or failed | Restarting isif 7 calc',\n",
    "#  'Job done | ISIF 3 done | --> isif 2',\n",
    "# ]\n",
    "\n",
    "# \"Error, need manual attention\"\n",
    "# \"Couldn't figure out what to do\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# row_i = df_static.iloc[1]\n",
    "\n",
    "# id_old = row_i[\"id_old\"]\n",
    "\n",
    "# id_old in ids_to_run_nersc\n",
    "\n",
    "# row_i\n",
    "# stoich_i = row_i[\"stoich\"]\n",
    "# id_old = row_i[\"id_old\"]\n",
    "\n",
    "\n",
    "# df_new_jobs_i = df_new_jobs[\n",
    "#     (df_new_jobs[\"stoich\"] == stoich_i) & \\\n",
    "#     (df_new_jobs[\"id\"] == str(id_old).zfill(3))\n",
    "#     ]\n",
    "\n",
    "# assert len(df_new_jobs_i) == 1, \"SIFISDF\"\n",
    "\n",
    "# row_j = df_new_jobs_i.iloc[0]\n",
    "\n",
    "# action_j = row_j[\"action\"]\n",
    "\n",
    "# bool_0 = action_j == 'Job is busy, will skip'\n",
    "# bool_1 = action_j == 'Time out or failed | Restarting isif 3 calc'\n",
    "# bool_2 = 'Time out or failed | Restarting isif 7 calc'\n",
    "# bool_3 = 'Job done | ISIF 3 done | --> isif 2'\n",
    "# if bool_0 or bool_1 or bool_2 or bool_3:\n",
    "#     tmp = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# ignore_ids_iro2\n",
    "\n",
    "# [i for i in set(ignore_ids_iro2) if i not in ignore_ids_iro2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i in ignore_ids_iro2:\n",
    "    \n",
    "#     if ignore_ids_iro2.count(i) > 1:\n",
    "#         print(i)\n",
    "# \"054\"\n",
    "\n",
    "# \"054\" in ignore_ids_sher_iro2\n",
    "\n",
    "# \"054\" in ignore_ids_nersc_iro2\n",
    "\n",
    "\n",
    "# ignore_ids_sher_iro2.count(\"054\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# 236 + 96 + 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# ids_0 = df_ab2_i[\"id\"].unique()\n",
    "\n",
    "# ids_0.shape\n",
    "# # df_ab2_i[\"id\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research-new]",
   "language": "python",
   "name": "conda-env-research-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
