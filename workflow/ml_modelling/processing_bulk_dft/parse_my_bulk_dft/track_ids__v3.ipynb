{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# #############################################################################\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path, unique_ids_path,\n",
    "    static_irox_structures_path)\n",
    "\n",
    "# #############################################################################\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from out_data.inputs_nersc_iro2 import ignore_ids as ignore_ids_nersc_iro2\n",
    "# from out_data.inputs_nersc_iro3 import ignore_ids as ignore_ids_nersc_iro3\n",
    "ignore_ids_nersc_iro3 = []\n",
    "\n",
    "from out_data.inputs_sher_iro2 import ignore_ids as ignore_ids_sher_iro2\n",
    "from out_data.inputs_sher_iro3 import ignore_ids as ignore_ids_sher_iro3\n",
    "\n",
    "from out_data.inputs_slac_iro2 import ignore_ids as ignore_ids_slac_iro2\n",
    "# from out_data.inputs_slac_iro3 import ignore_ids as ignore_ids_slac_iro3\n",
    "ignore_ids_slac_iro3 = []\n",
    "\n",
    "ignore_ids_iro2 = ignore_ids_nersc_iro2 + ignore_ids_sher_iro2 + ignore_ids_slac_iro2\n",
    "ignore_ids_iro3 = ignore_ids_nersc_iro3 + ignore_ids_sher_iro3 + ignore_ids_slac_iro3\n",
    "\n",
    "print(\"len(ignore_ids_iro2):\", len(ignore_ids_iro2))\n",
    "print(\"len(ignore_ids_iro3):\", len(ignore_ids_iro3))\n",
    "\n",
    "ignore_ids_dict = {\n",
    "    \"AB2\": ignore_ids_iro2,\n",
    "    \"AB3\": ignore_ids_iro3}\n",
    "\n",
    "\n",
    "# Pickling data ######################################################\n",
    "import os; import pickle\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"ignore_ids.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(ignore_ids_dict, fle)\n",
    "# #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from out_data.ids_to_run_nersc_iro2 import ids_to_run as ids_to_run_nersc_iro2\n",
    "from out_data.ids_to_run_nersc_iro3 import ids_to_run as ids_to_run_nersc_iro3\n",
    "\n",
    "from out_data.ids_to_run_slac_iro2 import ids_to_run as ids_to_run_slac_iro2\n",
    "# from out_data.ids_to_run_slac_iro3 import ids_to_run as ids_to_run_slac_iro3\n",
    "ids_to_run_slac_iro3 = []\n",
    "\n",
    "from out_data.ids_to_run_sher_iro2 import ids_to_run as ids_to_run_sher_iro2\n",
    "from out_data.ids_to_run_sher_iro3 import ids_to_run as ids_to_run_sher_iro3\n",
    "\n",
    "ids_to_run_iro2 = ids_to_run_nersc_iro2 + ids_to_run_sher_iro2 + ids_to_run_slac_iro2\n",
    "ids_to_run_iro3 = ids_to_run_nersc_iro3 + ids_to_run_sher_iro3 + ids_to_run_slac_iro3\n",
    "\n",
    "ids_to_run_iro3 = list(set(ids_to_run_iro3))\n",
    "ids_to_run_iro2 = list(set(ids_to_run_iro2))\n",
    "\n",
    "ids_to_run_sher = ids_to_run_sher_iro2 + ids_to_run_sher_iro3\n",
    "ids_to_run_nersc = ids_to_run_nersc_iro2 + ids_to_run_nersc_iro3\n",
    "ids_to_run_slac = ids_to_run_slac_iro2 + ids_to_run_slac_iro3\n",
    "\n",
    "ids_to_run_sher = list(set(ids_to_run_sher))\n",
    "ids_to_run_nersc = list(set(ids_to_run_nersc))\n",
    "ids_to_run_slac = list(set(ids_to_run_slac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoich_i = \"AB2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "    df_bulk_dft = pickle.load(fle)\n",
    "    df_bulk_dft = df_bulk_dft[(df_bulk_dft[\"source\"] == \"raul\")]\n",
    "\n",
    "with open(static_irox_structures_path, \"rb\") as fle:\n",
    "    df_static = pickle.load(fle)\n",
    "    df_static = df_static[(df_static[\"source\"] == \"chris\")]\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)\n",
    "\n",
    "with open(\"out_data/df_new_jobs.pickle\", \"rb\") as fle:\n",
    "    df_new_jobs = pickle.load(fle)\n",
    "\n",
    "with open(\"out_data/df_irox_long.pickle\", \"rb\") as fle:\n",
    "    df_irox_long = pickle.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids_in_process = df_new_jobs[df_new_jobs[\"stoich\"] == stoich_i][\"id\"]\n",
    "unique_ids_in_process = unique_ids_in_process.unique().tolist()\n",
    "unique_ids_in_process = [i for i in unique_ids_in_process if i.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    atoms = row_i[\"atoms\"]\n",
    "    num_atoms = atoms.get_number_of_atoms()\n",
    "    return(num_atoms)\n",
    "\n",
    "df_static[\"num_atoms\"] = df_static.apply(method, axis=1)\n",
    "df_static = df_static.drop([\"atoms\", \"path\", \"source\",], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    new_column_values_dict = {}\n",
    "\n",
    "    stoich_i = row_i[\"stoich\"]\n",
    "\n",
    "    id_old = row_i[\"id_old\"]\n",
    "    id_old_str = str(id_old).zfill(3)\n",
    "\n",
    "    # #########################################################################\n",
    "    if row_i.name in df_bulk_dft.index:\n",
    "        new_column_values_dict[\"done\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"done\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    if id_old_str in unique_ids_in_process:\n",
    "        new_column_values_dict[\"being_processed\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"being_processed\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    if stoich_i == \"AB2\":\n",
    "        ignore_ids_i = ignore_ids_iro2\n",
    "    elif stoich_i == \"AB3\":\n",
    "        ignore_ids_i = ignore_ids_iro3\n",
    "    # id_old = str(row_i[\"id_old\"]).zfill(3)\n",
    "\n",
    "    if id_old_str in ignore_ids_i:\n",
    "        new_column_values_dict[\"ignored\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"ignored\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    # stoich_i = row_i[\"stoich\"] == \"AB2\"\n",
    "    if stoich_i == \"AB2\":\n",
    "        ids_to_run_i = ids_to_run_iro2\n",
    "    elif stoich_i == \"AB3\":\n",
    "        ids_to_run_i = ids_to_run_iro3\n",
    "\n",
    "    if id_old in ids_to_run_i:\n",
    "        new_column_values_dict[\"ids_to_run\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"ids_to_run\"] = False\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    if stoich_i == \"AB2\":\n",
    "        if id_old in ids_to_run_sher_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"sherlock\"\n",
    "        elif id_old in ids_to_run_nersc_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"nersc\"\n",
    "        elif id_old in ids_to_run_slac_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"slac\"\n",
    "        else:\n",
    "            new_column_values_dict[\"cluster\"] = np.nan\n",
    "\n",
    "    elif stoich_i == \"AB3\":\n",
    "        if id_old in ids_to_run_sher_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"sherlock\"\n",
    "        elif id_old in ids_to_run_nersc_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"nersc\"\n",
    "        elif id_old in ids_to_run_slac_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"slac\"\n",
    "        else:\n",
    "            new_column_values_dict[\"cluster\"] = np.nan\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    df_new_jobs_i = df_new_jobs[\n",
    "        (df_new_jobs[\"stoich\"] == stoich_i) & \\\n",
    "        (df_new_jobs[\"id\"] == str(id_old).zfill(3))\n",
    "        ]\n",
    "\n",
    "    if len(df_new_jobs_i) == 1:\n",
    "        row_j = df_new_jobs_i.iloc[0]\n",
    "\n",
    "        action_j = row_j[\"action\"]\n",
    "\n",
    "        bool_0 = action_j == 'Job is busy, will skip'\n",
    "        bool_1 = action_j == 'Time out or failed | Restarting isif 3 calc'\n",
    "        bool_2 = action_j == 'Time out or failed | Restarting isif 7 calc'\n",
    "        bool_3 = action_j == 'Job done | ISIF 3 done | --> isif 2'\n",
    "        \n",
    "        if bool_0 or bool_1 or bool_2 or bool_3:\n",
    "            new_column_values_dict[\"running\"] = True\n",
    "        else:\n",
    "            new_column_values_dict[\"running\"] = False\n",
    "\n",
    "        bool_0 = action_j == \"Error, need manual attention\"\n",
    "        bool_1 = action_j == \"Couldn't figure out what to do\"\n",
    "        if bool_0 or bool_1:\n",
    "            new_column_values_dict[\"error\"] = True\n",
    "\n",
    "    elif len(df_new_jobs_i) == 0:\n",
    "        new_column_values_dict[\"running\"] = np.nan\n",
    "    elif len(df_new_jobs_i) > 1:\n",
    "        # print(df_new_jobs_i)\n",
    "        # display(df_new_jobs_i)\n",
    "        new_column_values_dict[\"running\"] = \"TEMP, more than 1 sys\"\n",
    "\n",
    "    # #########################################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "\n",
    "df_i = df_static\n",
    "df_static = df_i.apply(method, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "697 total AB2  structures in df_static\n",
    "\n",
    "------------------------------------\n",
    "87 systems with num_atoms >= 100\n",
    "\n",
    "132 systems with num_atoms >= 75"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "697 # Total AB2\n",
    "565 # AB2 | < 75 atoms\n",
    "105 # AB2 | < 75 atoms | Not  done\n",
    "# 32 # AB2 | < 75 atoms | Not  done | Not ignored\n",
    "18 # AB2 | < 75 atoms | Not  done | Not ignored\n",
    "\n",
    "\n",
    "# #####################################\n",
    "14  # Currently running\n",
    "7  # That have identified error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_static.drop(\"static_id\", axis=1)\n",
    "df = df[\n",
    "    (df[\"stoich\"] == stoich_i) & \\\n",
    "    (df[\"num_atoms\"] < 75) & \\\n",
    "#     (df[\"num_atoms\"] > 75) & \\\n",
    "#     (df[\"num_atoms\"] < 100) & \\\n",
    "#     (df[\"being_processed\"] == False) & \\\n",
    "    (df[\"done\"] == False) & \\\n",
    "    (df[\"ignored\"] == False) & \\\n",
    "#     (df[\"running\"] == True) & \\\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "print(\"df.shape:\", df.index.unique().shape)\n",
    "# display(df)\n",
    "\n",
    "\n",
    "\n",
    "from running_ids import sherlock_ids_running, slac_ids_running\n",
    "\n",
    "\n",
    "slac_ids_running\n",
    "\n",
    "print(len([i for i in df[\"id_old\"].tolist() if str(i).zfill(3) in sherlock_ids_running]))\n",
    "print(len([i for i in df[\"id_old\"].tolist() if str(i).zfill(3) in slac_ids_running]))\n",
    "\n",
    "# sherlock_ids_running\n",
    "# df[\"id_old\"].tolist()\n",
    "\n",
    "tmp_list_0 = np.sort(\n",
    "[str(i).zfill(3) for i in df[\"id_old\"].tolist() if str(i).zfill(3) not in sherlock_ids_running + slac_ids_running]\n",
    ")\n",
    "print(tmp_list_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_new_jobs[\n",
    "    (df_new_jobs[\"id\"].isin(tmp_list_0)) & \\\n",
    "    (df_new_jobs[\"stoich\"] == \"AB2\")\n",
    "    ].sort_values(\"id\")\n",
    "display(df_tmp); print(\"\")\n",
    "\n",
    "df = df_static\n",
    "df_static[\n",
    "    (df[\"id_old\"].isin(tmp_list_0)) & \\\n",
    "    (df[\"stoich\"] == \"AB2\") & \\\n",
    "    (df[\"error\"] != True)\n",
    "    ].sort_values(\"id_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60,\n",
    "# 89,\n",
    "# 118,\n",
    "155,\n",
    "193,\n",
    "# 236,\n",
    "# 250,\n",
    "275,\n",
    "303,\n",
    "# 483,\n",
    "# 649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_ids_list = []\n",
    "for i in ids_master_list:\n",
    "    if ids_master_list.count(i) > 1:\n",
    "        repeated_ids_list.append(i)\n",
    "\n",
    "\n",
    "set(repeated_ids_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research-new]",
   "language": "python",
   "name": "conda-env-research-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
