{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# #############################################################################\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path, unique_ids_path,\n",
    "    static_irox_structures_path)\n",
    "\n",
    "# #############################################################################\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from out_data.inputs_nersc_iro2 import ignore_ids as ignore_ids_nersc_iro2\n",
    "# from out_data.inputs_nersc_iro3 import ignore_ids as ignore_ids_nersc_iro3\n",
    "ignore_ids_nersc_iro3 = []\n",
    "\n",
    "from out_data.inputs_sher_iro2 import ignore_ids as ignore_ids_sher_iro2\n",
    "from out_data.inputs_sher_iro3 import ignore_ids as ignore_ids_sher_iro3\n",
    "\n",
    "from out_data.inputs_slac_iro2 import ignore_ids as ignore_ids_slac_iro2\n",
    "# from out_data.inputs_slac_iro3 import ignore_ids as ignore_ids_slac_iro3\n",
    "ignore_ids_slac_iro3 = []\n",
    "\n",
    "ignore_ids_iro2 = ignore_ids_nersc_iro2 + ignore_ids_sher_iro2 + ignore_ids_slac_iro2\n",
    "ignore_ids_iro3 = ignore_ids_nersc_iro3 + ignore_ids_sher_iro3 + ignore_ids_slac_iro3\n",
    "\n",
    "print(\"len(ignore_ids_iro2):\", len(ignore_ids_iro2))\n",
    "print(\"len(ignore_ids_iro3):\", len(ignore_ids_iro3))\n",
    "\n",
    "ignore_ids_dict = {\n",
    "    \"AB2\": ignore_ids_iro2,\n",
    "    \"AB3\": ignore_ids_iro3}\n",
    "\n",
    "\n",
    "# Pickling data ######################################################\n",
    "import os; import pickle\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"ignore_ids.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(ignore_ids_dict, fle)\n",
    "# #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from out_data.ids_to_run_nersc_iro2 import ids_to_run as ids_to_run_nersc_iro2\n",
    "from out_data.ids_to_run_nersc_iro3 import ids_to_run as ids_to_run_nersc_iro3\n",
    "\n",
    "from out_data.ids_to_run_slac_iro2 import ids_to_run as ids_to_run_slac_iro2\n",
    "# from out_data.ids_to_run_slac_iro3 import ids_to_run as ids_to_run_slac_iro3\n",
    "ids_to_run_slac_iro3 = []\n",
    "\n",
    "from out_data.ids_to_run_sher_iro2 import ids_to_run as ids_to_run_sher_iro2\n",
    "from out_data.ids_to_run_sher_iro3 import ids_to_run as ids_to_run_sher_iro3\n",
    "\n",
    "ids_to_run_iro2 = ids_to_run_nersc_iro2 + ids_to_run_sher_iro2 + ids_to_run_slac_iro2\n",
    "ids_to_run_iro3 = ids_to_run_nersc_iro3 + ids_to_run_sher_iro3 + ids_to_run_slac_iro3\n",
    "\n",
    "ids_to_run_iro3 = list(set(ids_to_run_iro3))\n",
    "ids_to_run_iro2 = list(set(ids_to_run_iro2))\n",
    "\n",
    "ids_to_run_sher = ids_to_run_sher_iro2 + ids_to_run_sher_iro3\n",
    "ids_to_run_nersc = ids_to_run_nersc_iro2 + ids_to_run_nersc_iro3\n",
    "ids_to_run_slac = ids_to_run_slac_iro2 + ids_to_run_slac_iro3\n",
    "\n",
    "ids_to_run_sher = list(set(ids_to_run_sher))\n",
    "ids_to_run_nersc = list(set(ids_to_run_nersc))\n",
    "ids_to_run_slac = list(set(ids_to_run_slac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "    df_bulk_dft = pickle.load(fle)\n",
    "    # df_bulk_dft = df_bulk_dft[(df_bulk_dft[\"source\"] == \"raul\")]\n",
    "\n",
    "with open(static_irox_structures_path, \"rb\") as fle:\n",
    "    df_static = pickle.load(fle)\n",
    "    df_static = df_static[(df_static[\"source\"] == \"chris\")]\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)\n",
    "\n",
    "with open(\"out_data/df_new_jobs.pickle\", \"rb\") as fle:\n",
    "    df_new_jobs = pickle.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_bulk_dft\n",
    "df_m[df_m[\"id_old\"] == 70]\n",
    "\n",
    "# bulk_dft_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    atoms = row_i[\"atoms\"]\n",
    "    num_atoms = atoms.get_number_of_atoms()\n",
    "    return(num_atoms)\n",
    "\n",
    "df_static[\"num_atoms\"] = df_static.apply(method, axis=1)\n",
    "df_static = df_static.drop([\"atoms\", \"path\", \"source\",], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_i = df_static.iloc[1]\n",
    "\n",
    "id_old = row_i[\"id_old\"]\n",
    "\n",
    "id_old in ids_to_run_nersc\n",
    "\n",
    "row_i\n",
    "stoich_i = row_i[\"stoich\"]\n",
    "id_old = row_i[\"id_old\"]\n",
    "\n",
    "\n",
    "df_new_jobs_i = df_new_jobs[\n",
    "    (df_new_jobs[\"stoich\"] == stoich_i) & \\\n",
    "    (df_new_jobs[\"id\"] == str(id_old).zfill(3))\n",
    "    ]\n",
    "\n",
    "assert len(df_new_jobs_i) == 1, \"SIFISDF\"\n",
    "\n",
    "row_j = df_new_jobs_i.iloc[0]\n",
    "\n",
    "action_j = row_j[\"action\"]\n",
    "\n",
    "bool_0 = action_j == 'Job is busy, will skip'\n",
    "bool_1 = action_j == 'Time out or failed | Restarting isif 3 calc'\n",
    "bool_2 = 'Time out or failed | Restarting isif 7 calc'\n",
    "bool_3 = 'Job done | ISIF 3 done | --> isif 2'\n",
    "if bool_0 or bool_1 or bool_2 or bool_3:\n",
    "    tmp = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_jobs[\"action\"].unique().tolist()\n",
    "\n",
    "[\n",
    " 'ALL DONE! | ISIF 2',\n",
    " 'Ignoring this id',\n",
    " 'Job is busy, will skip',\n",
    " 'Error, need manual attention',\n",
    " \"Couldn't figure out what to do\",\n",
    " 'Time out or failed | Restarting isif 3 calc',\n",
    " 'Time out or failed | Restarting isif 7 calc',\n",
    " 'Job done | ISIF 3 done | --> isif 2',\n",
    "]\n",
    "\n",
    "[\n",
    " 'ALL DONE! | ISIF 2',\n",
    " 'Ignoring this id',\n",
    " 'Job is busy, will skip',\n",
    " 'Error, need manual attention',\n",
    " \"Couldn't figure out what to do\",\n",
    " 'Time out or failed | Restarting isif 3 calc',\n",
    " 'Time out or failed | Restarting isif 7 calc',\n",
    " 'Job done | ISIF 3 done | --> isif 2',\n",
    "]\n",
    "\n",
    "\"Error, need manual attention\"\n",
    "\"Couldn't figure out what to do\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    new_column_values_dict = {}\n",
    "\n",
    "    stoich_i = row_i[\"stoich\"]\n",
    "    id_old = row_i[\"id_old\"]\n",
    "\n",
    "    # #########################################################################\n",
    "    if row_i.name in df_bulk_dft.index:\n",
    "        new_column_values_dict[\"done\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"done\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    if stoich_i == \"AB2\":\n",
    "        ignore_ids_i = ignore_ids_iro2\n",
    "    elif stoich_i == \"AB3\":\n",
    "        ignore_ids_i = ignore_ids_iro3\n",
    "    # id_old = str(row_i[\"id_old\"]).zfill(3)\n",
    "\n",
    "    if str(row_i[\"id_old\"]).zfill(3) in ignore_ids_i:\n",
    "        new_column_values_dict[\"ignored\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"ignored\"] = False\n",
    "\n",
    "    # #########################################################################\n",
    "    # stoich_i = row_i[\"stoich\"] == \"AB2\"\n",
    "    if stoich_i == \"AB2\":\n",
    "        ids_to_run_i = ids_to_run_iro2\n",
    "    elif stoich_i == \"AB3\":\n",
    "        ids_to_run_i = ids_to_run_iro3\n",
    "\n",
    "    if id_old in ids_to_run_i:\n",
    "        new_column_values_dict[\"ids_to_run\"] = True\n",
    "    else:\n",
    "        new_column_values_dict[\"ids_to_run\"] = False\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    if stoich_i == \"AB2\":\n",
    "        if id_old in ids_to_run_sher_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"sherlock\"\n",
    "        elif id_old in ids_to_run_nersc_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"nersc\"\n",
    "        elif id_old in ids_to_run_slac_iro2:\n",
    "            new_column_values_dict[\"cluster\"] = \"slac\"\n",
    "        else:\n",
    "            new_column_values_dict[\"cluster\"] = np.nan\n",
    "\n",
    "    elif stoich_i == \"AB3\":\n",
    "        if id_old in ids_to_run_sher_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"sherlock\"\n",
    "        elif id_old in ids_to_run_nersc_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"nersc\"\n",
    "        elif id_old in ids_to_run_slac_iro3:\n",
    "            new_column_values_dict[\"cluster\"] = \"slac\"\n",
    "        else:\n",
    "            new_column_values_dict[\"cluster\"] = np.nan\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    df_new_jobs_i = df_new_jobs[\n",
    "        (df_new_jobs[\"stoich\"] == stoich_i) & \\\n",
    "        (df_new_jobs[\"id\"] == str(id_old).zfill(3))\n",
    "        ]\n",
    "\n",
    "    if id_old == 122:\n",
    "        print(df_new_jobs_i)\n",
    "\n",
    "    # assert len(df_new_jobs_i) == 1, (stoich_i, id_old)\n",
    "\n",
    "    if len(df_new_jobs_i) == 1:\n",
    "        row_j = df_new_jobs_i.iloc[0]\n",
    "\n",
    "        action_j = row_j[\"action\"]\n",
    "\n",
    "        bool_0 = action_j == 'Job is busy, will skip'\n",
    "        bool_1 = action_j == 'Time out or failed | Restarting isif 3 calc'\n",
    "        bool_2 = action_j == 'Time out or failed | Restarting isif 7 calc'\n",
    "        bool_3 = action_j == 'Job done | ISIF 3 done | --> isif 2'\n",
    "        \n",
    "        if id_old == 122:\n",
    "            print(bool_0)\n",
    "            print(bool_1)\n",
    "            print(bool_2)\n",
    "            print(bool_3)\n",
    "            \n",
    "        if bool_0 or bool_1 or bool_2 or bool_3:\n",
    "            new_column_values_dict[\"running\"] = True\n",
    "        else:\n",
    "            new_column_values_dict[\"running\"] = False\n",
    "\n",
    "        bool_0 = action_j == \"Error, need manual attention\"\n",
    "        bool_1 = action_j == \"Couldn't figure out what to do\"\n",
    "        if bool_0 or bool_1:\n",
    "            new_column_values_dict[\"error\"] = True\n",
    "\n",
    "    elif len(df_new_jobs_i) == 0:\n",
    "        new_column_values_dict[\"running\"] = np.nan\n",
    "    elif len(df_new_jobs_i) > 1:\n",
    "        from IPython.display import display\n",
    "        # print(df_new_jobs_i)\n",
    "        display(df_new_jobs_i)\n",
    "        new_column_values_dict[\"running\"] = \"TEMP, more than 1 sys\"\n",
    "\n",
    "    # #########################################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "\n",
    "df_i = df_static\n",
    "df_static = df_i.apply(method, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bulk_dft[df_bulk_dft[\"stoich\"] == \"AB2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static.loc[\"9k7pvd7izl\"]\n",
    "\n",
    "[250, 168, 236, 280, 311, 22, 182]\n",
    "\n",
    "# df_new_jobs_i =\n",
    "df_new_jobs[\n",
    "    (df_new_jobs[\"stoich\"] == \"AB2\") & \\\n",
    "    (df_new_jobs[\"id\"] == str(122).zfill(3))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "    (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].shape\n",
    "\n",
    "# [\"id_old\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_ab2_systems = \n",
    "df_static[df_static[\"stoich\"] == \"AB2\"].shape[0]\n",
    "\n",
    "# Total AB2 systems\n",
    "697\n",
    "\n",
    "# #############################################################################\n",
    "# Finished AB2 systems\n",
    "283\n",
    "\n",
    "# 100 atoms or greater\n",
    "87\n",
    "\n",
    "# Manually ignored\n",
    "172\n",
    "\n",
    "# Running jobs\n",
    "56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_ab2_systems = \n",
    "df_static[df_static[\"stoich\"] == \"AB3\"].shape[0]\n",
    "\n",
    "# # Total AB2 systems\n",
    "# 697\n",
    "\n",
    "# # #############################################################################\n",
    "# # Finished AB2 systems\n",
    "# 283\n",
    "\n",
    "# # 100 atoms or greater\n",
    "# 87\n",
    "\n",
    "# # Manually ignored\n",
    "# 172\n",
    "\n",
    "# # Running jobs\n",
    "# 56\n",
    "\n",
    "# # Errored\n",
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systems that have not been submitted yet\n",
    "697 - (283 + 87 + 172 + 56 + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total AB3\n",
    "\n",
    "259\n",
    "\n",
    "# Finished\n",
    "246\n",
    "\n",
    "# 100 atoms or greater\n",
    "2\n",
    "\n",
    "# Ignored\n",
    "0\n",
    "\n",
    "# Errored\n",
    "5\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "246 + 2 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ab2_ids = df_static[\n",
    "    (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "\n",
    "ab2_completed_ids = df_static[\n",
    "    (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "#     (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "ab2_errored_ids = df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "    (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "\n",
    "ab2_large_atoms_ids = df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "    (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "#     (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB2\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "\n",
    "ab2_ids_accounted_for = ab2_completed_ids + ab2_errored_ids + ab2_large_atoms_ids\n",
    "\n",
    "\n",
    "ab2_ids_not_acc = [i for i in all_ab2_ids if i not in ab2_ids_accounted_for]\n",
    "# [i for i all_ab3_ids]\n",
    "\n",
    "# df_new_jobs\n",
    "\n",
    "df_static.loc[ab2_ids_not_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ab3_ids = df_static[\n",
    "    (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "\n",
    "ab3_completed_ids = df_static[\n",
    "    (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "#     (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "ab3_errored_ids = df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "#     (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "    (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "\n",
    "ab3_large_atoms_ids = df_static[\n",
    "#     (df_static[\"done\"] == True) & \\\n",
    "#     (df_static[\"ignored\"] == True) & \\\n",
    "    (df_static[\"num_atoms\"] >= 100) & \\\n",
    "#     (df_static[\"ids_to_run\"] == False) & \\\n",
    "#     (df_static[\"running\"] == True) & \\\n",
    "#     (df_static[\"error\"] == True) & \\\n",
    "    (df_static[\"stoich\"] == \"AB3\") & \\\n",
    "    [True for i in range(len(df_static))]\n",
    "    ].index.tolist()\n",
    "\n",
    "\n",
    "ab3_ids_accounted_for = ab3_completed_ids + ab3_errored_ids + ab3_large_atoms_ids\n",
    "\n",
    "\n",
    "ab3_ids_not_acc = [i for i in all_ab3_ids if i not in ab3_ids_accounted_for]\n",
    "# [i for i all_ab3_ids]\n",
    "\n",
    "# df_new_jobs\n",
    "\n",
    "df_static.loc[ab3_ids_not_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_jobs[df_new_jobs[\"id\"] == \"070\"]\n",
    "# \"7av2zgmqvp\"\n",
    "\n",
    "# df_bulk_dft[df_bulk_dft[\"id_old\"] == 70]\n",
    "\n",
    "df_m = df_bulk_dft\n",
    "df_m[df_m[\"id_old\"] == 70]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research-new]",
   "language": "python",
   "name": "conda-env-research-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
