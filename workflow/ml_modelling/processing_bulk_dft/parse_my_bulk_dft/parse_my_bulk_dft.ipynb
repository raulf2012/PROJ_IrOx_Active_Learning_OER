{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing my bulk DFT data\n",
    "---\n",
    "\n",
    "130 calculations with the previous criteria by which jobs are included\n",
    "103 after requiring the job to be done 100%"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ase import io\n",
    "import pandas as pd\n",
    "\n",
    "from misc_modules.pandas_methods import drop_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Prep"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 'out_data' dir\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IrO2 Bulk Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # #############################################################################\n",
    "# Parsing Sherlock IrO2 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro2\",\n",
    "    \"df_dict_nersc.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "    df_new_jobs_nersc_iro2 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_nersc_iro2[\"source\"] = \"nersc\"\n",
    "\n",
    "    df_iro2_nersc = df_dict[\"df\"]\n",
    "    df_iro2_nersc[\"source\"] = \"nersc\"\n",
    "\n",
    "# #############################################################################\n",
    "# Parsing Sherlock IrO2 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro2\",\n",
    "    \"df_dict_sher.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "    df_new_jobs_sher_iro2 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_sher_iro2[\"source\"] = \"sherlock\"\n",
    "\n",
    "    df_iro2_sherlock = df_dict[\"df\"]\n",
    "    df_iro2_sherlock[\"source\"] = \"sherlock\"\n",
    "\n",
    "# #############################################################################\n",
    "# Parsing SLAC IrO2 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro2\",\n",
    "    \"df_dict_slac.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "\n",
    "    df_new_jobs_slac_iro2 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_slac_iro2[\"source\"] = \"slac\"\n",
    "\n",
    "    df_iro2_slac = df_dict[\"df\"]\n",
    "    df_iro2_slac[\"source\"] = \"slac\"\n",
    "\n",
    "# #############################################################################\n",
    "df_new_jobs_iro2 = pd.concat([\n",
    "    df_new_jobs_nersc_iro2,\n",
    "    df_new_jobs_sher_iro2,\n",
    "    df_new_jobs_slac_iro2,\n",
    "    ])\n",
    "df_new_jobs_iro2[\"stoich\"] = \"AB2\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "df_iro2_long = pd.concat([\n",
    "    df_iro2_nersc,\n",
    "    df_iro2_sherlock,\n",
    "    df_iro2_slac,\n",
    "    ], axis=0, sort=True)\n",
    "\n",
    "df_iro2_long[\"stoich\"] = \"AB2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_iro2_long.head()\n",
    "\n",
    "df_iro2_long[df_iro2_long[\"id\"] == \"338\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "grouped = df_iro2_long.groupby([\"id\"])\n",
    "for name, group in grouped:\n",
    "\n",
    "    # df_succ = group[group[\"job_state\"] == \"SUCCEEDED\"]\n",
    "    df_succ = group[group[\"completed\"] == True]\n",
    "    isif_2_done = 2 in df_succ[\"isif\"].tolist()\n",
    "\n",
    "    if len(df_succ) > 0 and isif_2_done:\n",
    "        latest_succ_rev = df_succ.sort_values(\"revision\").iloc[-1]\n",
    "        data_list.append(latest_succ_rev)\n",
    "df_iro2 = pd.DataFrame(data_list)\n",
    "\n",
    "# Droping all unnecessary columns\n",
    "df_iro2 = drop_columns(df=df_iro2, columns=[\"atoms\", \"path\"], keep_or_drop=\"keep\")\n",
    "\n",
    "# Adding stoich column\n",
    "df_iro2[\"stoich\"] = \"AB2\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #############################################################################\n",
    "# #############################################################################\n",
    "# #############################################################################\n",
    "# #############################################################################"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IrO3 Bulk Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Parsing NERSC IrO3 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro3\",\n",
    "    \"df_dict_nersc.pickle\")\n",
    "    # \"df_dict.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "\n",
    "    df_new_jobs_nersc_iro3 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_nersc_iro3[\"source\"] = \"nersc\"\n",
    "\n",
    "    df_iro3_nersc = df_dict[\"df\"]\n",
    "    df_iro3_nersc[\"source\"] = \"nersc\"\n",
    "\n",
    "# #############################################################################\n",
    "# Parsing Sherlock IrO3 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro3\",\n",
    "    \"df_dict_sher.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "\n",
    "    df_new_jobs_sher_iro3 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_sher_iro3[\"source\"] = \"sherlock\"\n",
    "\n",
    "    df_iro3_sher = df_dict[\"df\"]\n",
    "    df_iro3_sher[\"source\"] = \"sherlock\"\n",
    "\n",
    "# #############################################################################\n",
    "df_new_jobs_iro3 = pd.concat([\n",
    "    df_new_jobs_nersc_iro3,\n",
    "    df_new_jobs_sher_iro3,\n",
    "    # df_new_jobs_slac_iro2,\n",
    "    ])\n",
    "df_new_jobs_iro3[\"stoich\"] = \"AB3\"\n",
    "\n",
    "# #############################################################################\n",
    "df_iro3_long = pd.concat([\n",
    "    df_iro3_nersc,\n",
    "    df_iro3_sher,\n",
    "    ], axis=0, sort=True)\n",
    "\n",
    "df_iro3_long[\"stoich\"] = \"AB3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing IrO3 Dataframe\n",
    "data_list = []\n",
    "grouped = df_iro3_long.groupby([\"pre_path\"])\n",
    "for name, group in grouped:\n",
    "\n",
    "    # df_succ = group[group[\"job_state\"] == \"SUCCEEDED\"]\n",
    "    df_succ = group[group[\"completed\"] == True]\n",
    "\n",
    "    isif_2_done = 2 in df_succ[\"isif\"].tolist()\n",
    "\n",
    "    if len(df_succ) > 0 and isif_2_done:\n",
    "        latest_succ_rev = df_succ.sort_values(\"revision\").iloc[-1]\n",
    "        data_list.append(latest_succ_rev)\n",
    "df_iro3 = pd.DataFrame(data_list)\n",
    "\n",
    "# Droping all unnecessary columns\n",
    "df_iro3 = drop_columns(df=df_iro3, columns=[\"atoms\", \"path\"], keep_or_drop=\"keep\")\n",
    "\n",
    "# Adding stoich column\n",
    "df_iro3[\"stoich\"] = \"AB3\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #############################################################################\n",
    "# #############################################################################\n",
    "# #############################################################################\n",
    "# #############################################################################"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining dataframes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "frames = [df_iro2, df_iro3]\n",
    "df_m = pd.concat(frames)\n",
    "print(\"df_m.shape:\", df_m.shape)\n",
    "\n",
    "# #############################################################################\n",
    "frames = [df_new_jobs_iro2, df_new_jobs_iro3]\n",
    "df_new_jobs = pd.concat(frames)\n",
    "print(\"df_new_jobs.shape:\", df_new_jobs.shape)\n",
    "\n",
    "# #############################################################################\n",
    "frames = [df_iro2_long, df_iro3_long]\n",
    "df_irox_long = pd.concat(frames)\n",
    "print(\"df_irox_long.shape:\", df_irox_long.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing ID from path"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_i = df_m.iloc[0]\n",
    "\n",
    "def method(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    path_i = row_i[\"path\"]\n",
    "    folder_lists__isdigit = [i for i in path_i.split(\"/\") if i.isdigit()]\n",
    "\n",
    "    mess_i = \"Must have only one folder in path that is numeric\"\n",
    "    assert len(folder_lists__isdigit) == 1, mess_i\n",
    "\n",
    "    id_i =int(folder_lists__isdigit[0])\n",
    "\n",
    "    return(id_i)\n",
    "\n",
    "df_m[\"id_old\"] = df_m.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(directory, \"df_bulk_raul_iro2.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_iro2, fle)\n",
    "    \n",
    "with open(os.path.join(directory, \"df_bulk_raul_iro3.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_iro3, fle)\n",
    "\n",
    "with open(os.path.join(directory, \"df_bulk_raul_irox.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_m, fle)\n",
    "\n",
    "# #############################################################################\n",
    "with open(os.path.join(directory, \"df_new_jobs.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_new_jobs, fle)\n",
    "\n",
    "with open(os.path.join(directory, \"df_irox_long.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_irox_long, fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AB2:\", df_m[df_m[\"stoich\"] == \"AB2\"].shape)\n",
    "print(\"AB3:\", df_m[df_m[\"stoich\"] == \"AB3\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping track of competed jobs (Manually)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# IrO2\n",
    "# ########################\n",
    "\n",
    "(368, 4)\n",
    "(442, 4)\n",
    "(445, 4)\n",
    "(446, 4)\n",
    "(448, 4)\n",
    "(450, 4)\n",
    "(454, 4)\n",
    "(460, 4)\n",
    "(463, 4)\n",
    "(468, 4)\n",
    "(470, 4)\n",
    "(475, 4)\n",
    "(478, 4)\n",
    "(480, 4)\n",
    "(482, 4)\n",
    "(484, 4)\n",
    "(485, 4)\n",
    "(487, 4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# IrO3\n",
    "# ########################\n",
    "\n",
    "(238, 4)\n",
    "(241, 4)\n",
    "(244, 4)\n",
    "(245, 4)\n",
    "(246, 4)\n",
    "(248, 4)\n",
    "(249, 4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# IrO2\n",
    "# ########################\n",
    "\n",
    "(99, 4)\n",
    "(100, 4)\n",
    "(102, 4)\n",
    "(106, 4)\n",
    "(108, 4)\n",
    "(131, 4)\n",
    "(227, 4)\n",
    "(230, 4)\n",
    "(231, 4)\n",
    "(233, 4)\n",
    "(235, 4)\n",
    "(236, 4)\n",
    "(237, 4)\n",
    "(239, 4)\n",
    "(242, 4)\n",
    "(244, 4)\n",
    "(245, 4)\n",
    "(247, 4)\n",
    "(250, 4)\n",
    "(253, 4)\n",
    "(254, 4)\n",
    "(256, 4)\n",
    "(258, 4)\n",
    "(266, 4)\n",
    "(272, 4)\n",
    "(273, 4)\n",
    "(276, 4)\n",
    "(279, 4)\n",
    "(283, 4)\n",
    "(285, 4)\n",
    "(287, 4)\n",
    "(289, 4)\n",
    "(292, 4)\n",
    "(295, 4)\n",
    "(297, 4)\n",
    "(305, 4)\n",
    "(310, 4)\n",
    "(313, 4)\n",
    "(317, 4)\n",
    "(322, 4)\n",
    "(326, 4)\n",
    "(330, 4)\n",
    "(335, 4)\n",
    "(340, 4)\n",
    "(354, 4)\n",
    "(357, 4)\n",
    "(359, 4)\n",
    "(367, 4)\n",
    "\n",
    "# IrO3\n",
    "# ########################\n",
    "\n",
    "(37, 4)\n",
    "(47, 4)\n",
    "(55, 4)\n",
    "(60, 4)\n",
    "(62, 4)\n",
    "(87, 4)\n",
    "(116, 4)\n",
    "(119, 4)\n",
    "(120, 4)\n",
    "(121, 4)\n",
    "(123, 4)\n",
    "(124, 4)\n",
    "(127, 4)\n",
    "(128, 4)\n",
    "(132, 4)\n",
    "(135, 4)\n",
    "(146, 4)\n",
    "(150, 4)\n",
    "(155, 4)\n",
    "(158, 4)\n",
    "(161, 4)\n",
    "(177, 4)\n",
    "(188, 4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iro2_slac\n",
    "pd.set_option('display.max_rows', None)\n",
    "# 13 | errored\n",
    "# 58 | finished\n",
    "df_tmp = df_new_jobs_slac_iro2\n",
    "\n",
    "df_tmp = df_tmp[df_tmp[\"action\"] != \"ALL DONE! | ISIF 2\"]\n",
    "\n",
    "df_tmp = df_tmp[df_tmp[\"action\"] != \"Job is busy, will skip\"]\n",
    "print(\"df_tmp.shape:\", df_tmp.shape)\n",
    "display(df_tmp)\n",
    "\n",
    "print(\"All jobs:\", df_new_jobs_slac_iro2.shape[0], \"\\n\")\n",
    "print(\"Busy jobs:\", df_new_jobs_slac_iro2[df_new_jobs_slac_iro2[\"action\"] == \"Job is busy, will skip\"].shape[0])\n",
    "print(\"Finished jobs:\", df_new_jobs_slac_iro2[df_new_jobs_slac_iro2[\"action\"] == \"ALL DONE! | ISIF 2\"].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_iro2_slac\n",
    "\n",
    "df_new_jobs_slac_iro2[df_new_jobs_slac_iro2[\"id\"] == \"590\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in df_new_jobs_sher_iro2[\"pre_path\"].tolist() if \"iro3\" in i]\n",
    "\n",
    "[i for i in df_iro2_sherlock[\"path\"].tolist() if \"iro3\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_m[df_m[\"stoich\"] == \"AB2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m[df_m[\"id_old\"] == 182]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox] *",
   "language": "python",
   "name": "conda-env-PROJ_irox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
