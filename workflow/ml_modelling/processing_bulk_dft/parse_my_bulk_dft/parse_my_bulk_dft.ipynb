{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing my bulk DFT data\n",
    "---\n",
    "\n",
    "130 calculations with the previous criteria by which jobs are included\n",
    "103 after requiring the job to be done 100%"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ase import io\n",
    "import pandas as pd\n",
    "\n",
    "from misc_modules.pandas_methods import drop_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Prep"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IrO2 Bulk Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # #############################################################################\n",
    "# Parsing Sherlock IrO2 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro2\",\n",
    "    \"df_dict_nersc.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "    df_new_jobs_nersc_iro2 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_nersc_iro2[\"source\"] = \"nersc\"\n",
    "\n",
    "    df_iro2_nersc = df_dict[\"df\"]\n",
    "    df_iro2_nersc[\"source\"] = \"nersc\"\n",
    "\n",
    "# #############################################################################\n",
    "# Parsing Sherlock IrO2 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro2\",\n",
    "    \"df_dict_sher.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "    df_new_jobs_sher_iro2 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_sher_iro2[\"source\"] = \"sherlock\"\n",
    "\n",
    "    df_iro2_sherlock = df_dict[\"df\"]\n",
    "    df_iro2_sherlock[\"source\"] = \"sherlock\"\n",
    "\n",
    "# #############################################################################\n",
    "# Parsing SLAC IrO2 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro2\",\n",
    "    \"df_dict_slac.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "\n",
    "    df_new_jobs_slac_iro2 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_slac_iro2[\"source\"] = \"slac\"\n",
    "\n",
    "    df_iro2_slac = df_dict[\"df\"]\n",
    "    df_iro2_slac[\"source\"] = \"slac\"\n",
    "\n",
    "# #############################################################################\n",
    "df_new_jobs_iro2 = pd.concat([\n",
    "    df_new_jobs_nersc_iro2,\n",
    "    df_new_jobs_sher_iro2,\n",
    "    df_new_jobs_slac_iro2,\n",
    "    ])\n",
    "df_new_jobs_iro2[\"stoich\"] = \"AB2\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "df_iro2_long = pd.concat([\n",
    "    df_iro2_nersc,\n",
    "    df_iro2_sherlock,\n",
    "    df_iro2_slac,\n",
    "    ], axis=0, sort=True)\n",
    "\n",
    "df_iro2_long[\"stoich\"] = \"AB2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_iro2_long.head()\n",
    "\n",
    "# df_iro2_long[df_iro2_long[\"id\"] == \"338\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "grouped = df_iro2_long.groupby([\"id\"])\n",
    "for name, group in grouped:\n",
    "\n",
    "    # df_succ = group[group[\"job_state\"] == \"SUCCEEDED\"]\n",
    "    df_succ = group[group[\"completed\"] == True]\n",
    "    isif_2_done = 2 in df_succ[\"isif\"].tolist()\n",
    "\n",
    "    if len(df_succ) > 0 and isif_2_done:\n",
    "        latest_succ_rev = df_succ.sort_values(\"revision\").iloc[-1]\n",
    "        data_list.append(latest_succ_rev)\n",
    "df_iro2 = pd.DataFrame(data_list)\n",
    "\n",
    "# Droping all unnecessary columns\n",
    "df_iro2 = drop_columns(df=df_iro2, columns=[\"atoms\", \"path\"], keep_or_drop=\"keep\")\n",
    "\n",
    "# Adding stoich column\n",
    "df_iro2[\"stoich\"] = \"AB2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IrO3 Bulk Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Parsing NERSC IrO3 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro3\",\n",
    "    \"df_dict_nersc.pickle\")\n",
    "    # \"df_dict.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "\n",
    "    df_new_jobs_nersc_iro3 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_nersc_iro3[\"source\"] = \"nersc\"\n",
    "\n",
    "    df_iro3_nersc = df_dict[\"df\"]\n",
    "    df_iro3_nersc[\"source\"] = \"nersc\"\n",
    "\n",
    "# #############################################################################\n",
    "# Parsing Sherlock IrO3 DFT Data\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_DATA\"],\n",
    "    \"04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro3\",\n",
    "    \"df_dict_sher.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_dict = pickle.load(fle)\n",
    "\n",
    "    df_new_jobs_sher_iro3 = df_dict[\"df_new_jobs\"]\n",
    "    df_new_jobs_sher_iro3[\"source\"] = \"sherlock\"\n",
    "\n",
    "    df_iro3_sher = df_dict[\"df\"]\n",
    "    df_iro3_sher[\"source\"] = \"sherlock\"\n",
    "\n",
    "# #############################################################################\n",
    "df_new_jobs_iro3 = pd.concat([\n",
    "    df_new_jobs_nersc_iro3,\n",
    "    df_new_jobs_sher_iro3,\n",
    "    # df_new_jobs_slac_iro2,\n",
    "    ])\n",
    "df_new_jobs_iro3[\"stoich\"] = \"AB3\"\n",
    "\n",
    "# #############################################################################\n",
    "df_iro3_long = pd.concat([\n",
    "    df_iro3_nersc,\n",
    "    df_iro3_sher,\n",
    "    ], axis=0, sort=True)\n",
    "\n",
    "df_iro3_long[\"stoich\"] = \"AB3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing IrO3 Dataframe\n",
    "data_list = []\n",
    "grouped = df_iro3_long.groupby([\"pre_path\"])\n",
    "for name, group in grouped:\n",
    "\n",
    "    # df_succ = group[group[\"job_state\"] == \"SUCCEEDED\"]\n",
    "    df_succ = group[group[\"completed\"] == True]\n",
    "\n",
    "    isif_2_done = 2 in df_succ[\"isif\"].tolist()\n",
    "\n",
    "    if len(df_succ) > 0 and isif_2_done:\n",
    "        latest_succ_rev = df_succ.sort_values(\"revision\").iloc[-1]\n",
    "        data_list.append(latest_succ_rev)\n",
    "df_iro3 = pd.DataFrame(data_list)\n",
    "\n",
    "# Droping all unnecessary columns\n",
    "df_iro3 = drop_columns(df=df_iro3, columns=[\"atoms\", \"path\"], keep_or_drop=\"keep\")\n",
    "\n",
    "# Adding stoich column\n",
    "df_iro3[\"stoich\"] = \"AB3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining dataframes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "frames = [df_iro2, df_iro3]\n",
    "df_m = pd.concat(frames)\n",
    "print(\"df_m.shape:\", df_m.shape)\n",
    "\n",
    "# #############################################################################\n",
    "frames = [df_new_jobs_iro2, df_new_jobs_iro3]\n",
    "df_new_jobs = pd.concat(frames)\n",
    "print(\"df_new_jobs.shape:\", df_new_jobs.shape)\n",
    "\n",
    "# #############################################################################\n",
    "frames = [df_iro2_long, df_iro3_long]\n",
    "df_irox_long = pd.concat(frames)\n",
    "print(\"df_irox_long.shape:\", df_irox_long.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing ID from path"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_i = df_m.iloc[0]\n",
    "\n",
    "def method(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    path_i = row_i[\"path\"]\n",
    "    folder_lists__isdigit = [i for i in path_i.split(\"/\") if i.isdigit()]\n",
    "\n",
    "    mess_i = \"Must have only one folder in path that is numeric\"\n",
    "    assert len(folder_lists__isdigit) == 1, mess_i\n",
    "\n",
    "    id_i =int(folder_lists__isdigit[0])\n",
    "\n",
    "    return(id_i)\n",
    "\n",
    "df_m[\"id_old\"] = df_m.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(directory, \"df_bulk_raul_iro2.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_iro2, fle)\n",
    "    \n",
    "with open(os.path.join(directory, \"df_bulk_raul_iro3.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_iro3, fle)\n",
    "\n",
    "with open(os.path.join(directory, \"df_bulk_raul_irox.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_m, fle)\n",
    "\n",
    "# #############################################################################\n",
    "with open(os.path.join(directory, \"df_new_jobs.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_new_jobs, fle)\n",
    "\n",
    "with open(os.path.join(directory, \"df_irox_long.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_irox_long, fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AB2:\", df_m[df_m[\"stoich\"] == \"AB2\"].shape)\n",
    "print(\"AB3:\", df_m[df_m[\"stoich\"] == \"AB3\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iro2_slac\n",
    "pd.set_option('display.max_rows', None)\n",
    "# 13 | errored\n",
    "# 58 | finished\n",
    "df_tmp = df_new_jobs_slac_iro2\n",
    "\n",
    "df_tmp = df_tmp[df_tmp[\"action\"] != \"ALL DONE! | ISIF 2\"]\n",
    "\n",
    "df_tmp = df_tmp[df_tmp[\"action\"] != \"Job is busy, will skip\"]\n",
    "print(\"df_tmp.shape:\", df_tmp.shape)\n",
    "display(df_tmp)\n",
    "\n",
    "print(\"All jobs:\", df_new_jobs_slac_iro2.shape[0], \"\\n\")\n",
    "print(\"Busy jobs:\", df_new_jobs_slac_iro2[df_new_jobs_slac_iro2[\"action\"] == \"Job is busy, will skip\"].shape[0])\n",
    "print(\"Finished jobs:\", df_new_jobs_slac_iro2[df_new_jobs_slac_iro2[\"action\"] == \"ALL DONE! | ISIF 2\"].shape[0])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox]",
   "language": "python",
   "name": "conda-env-PROJ_irox-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
