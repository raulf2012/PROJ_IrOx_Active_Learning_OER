{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New ML Active Learning Workflow\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following system is failing with voro fingerprinting:\n",
    "z39g648rnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_PROJ_DATA = False\n",
    "read_from_PROJ_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_Active_Learning_OER_test_1/workflow/ml_modelling/voronoi_featurize\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "from protosearch.ml_modelling.fingerprint import (\n",
    "    FingerPrint,\n",
    "    VoronoiFingerprint\n",
    "    )\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path,\n",
    "    unique_ids_path,\n",
    "    prototypes_data_path,\n",
    "    static_irox_structures_path,\n",
    "    static_irox_structures_kirsten_path,\n",
    "    df_features_pre_opt_path,\n",
    "    df_features_pre_opt_kirsten_path,\n",
    "    df_features_post_opt_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "    df_bulk_dft = pickle.load(fle)\n",
    "\n",
    "with open(static_irox_structures_path, \"rb\") as fle:\n",
    "    df_struct = pickle.load(fle)\n",
    "\n",
    "# with open(static_irox_structures_kirsten_path, \"rb\") as fle:\n",
    "#     df_struct_kirsten = pickle.load(fle)\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove rows with missing atoms objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing missing data\n",
    "df_bulk_dft = df_bulk_dft[df_bulk_dft[\"atoms\"].notnull()]\n",
    "df_struct = df_struct[df_struct[\"atoms\"].notnull()]\n",
    "\n",
    "df_bulk_dft[\"INDEX_NEW\"] = df_bulk_dft.index + \"_\" + df_bulk_dft[\"source\"]\n",
    "df_bulk_dft[\"INDEX_OLD\"] = df_bulk_dft.index\n",
    "df_bulk_dft = df_bulk_dft.set_index(\"INDEX_NEW\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Current df_features_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_from_PROJ_DATA:\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_DATA\"], \"04_IrOx_surfaces_OER\",\n",
    "        \"PROJECT_COMPUTED_OUT_DATA/PROJ_IrOx_Active_Learning_OER\",\n",
    "        \"workflow/ml_modelling/voronoi_featurize\",\n",
    "        \"out_data/df_features_pre_opt.pickle\")\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_features_pre_opt__before = pickle.load(fle)\n",
    "else:\n",
    "    try:\n",
    "        with open(df_features_pre_opt_path, \"rb\") as fle:\n",
    "            df_features_pre_opt__before = pickle.load(fle)\n",
    "    except:\n",
    "        df_features_pre_opt__before = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_process = [i for i in df_struct.index if i not in df_features_pre_opt__before.index]\n",
    "\n",
    "df_struct_to_process = df_struct.loc[ids_to_process]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pre-opt Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_struct_to_process.shape[0] == 0:\n",
    "    df_features_pre_opt__after = pd.DataFrame()\n",
    "else:\n",
    "    FP_struct = FingerPrint(**{\n",
    "        \"feature_methods\": [\"voronoi\"],\n",
    "        \"input_data\": df_struct_to_process,\n",
    "        \"input_index\": [\"atoms\"]})\n",
    "\n",
    "    FP_struct.generate_fingerprints()\n",
    "    df_features_pre_opt__after = FP_struct.fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_pre_opt = pd.concat([\n",
    "    df_features_pre_opt__after,\n",
    "    df_features_pre_opt__before,\n",
    "    ])\n",
    "\n",
    "# #############################################################################\n",
    "with open(df_features_pre_opt_path, \"wb\") as fle:\n",
    "    pickle.dump(df_features_pre_opt, fle)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Features for Post-DFT Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_from_PROJ_DATA:\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_DATA\"], \"04_IrOx_surfaces_OER\",\n",
    "        \"PROJECT_COMPUTED_OUT_DATA/PROJ_IrOx_Active_Learning_OER\",\n",
    "        \"workflow/ml_modelling/voronoi_featurize\",\n",
    "        \"out_data/df_features_post_opt.pickle\")\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_features_post_opt = pickle.load(fle)\n",
    "\n",
    "        df_features_post_opt[\"data\", \"INDEX_OLD\"] = df_features_post_opt.index\n",
    "        df_features_post_opt[\"data\", \"INDEX_NEW\"] = df_features_post_opt[\"data\", \"INDEX_OLD\"] + \"_\" + df_features_post_opt[\"data\"][\"source\"]\n",
    "\n",
    "        df_features_post_opt = df_features_post_opt.set_index(df_features_post_opt[\"data\", \"INDEX_NEW\"])\n",
    "        df_features_post_opt = df_features_post_opt.drop(labels=[[\"data\", \"INDEX_NEW\"]], axis=1)\n",
    "else:\n",
    "\n",
    "    try:\n",
    "        with open(df_features_post_opt_path, \"rb\") as fle:\n",
    "            df_features_post_opt = pickle.load(fle)\n",
    "\n",
    "        df_features_post_opt[\"data\", \"INDEX_OLD\"] = df_features_post_opt.index\n",
    "        df_features_post_opt[\"data\", \"INDEX_NEW\"] = df_features_post_opt[\"data\", \"INDEX_OLD\"] + \"_\" + df_features_post_opt[\"data\"][\"source\"]\n",
    "\n",
    "        df_features_post_opt = df_features_post_opt.set_index(df_features_post_opt[\"data\", \"INDEX_NEW\"])\n",
    "        df_features_post_opt = df_features_post_opt.drop(labels=[[\"data\", \"INDEX_NEW\"]], axis=1)\n",
    "\n",
    "        print(\"df_features_post_opt.shape:\", df_features_post_opt.shape)\n",
    "\n",
    "    except:\n",
    "        df_features_post_opt = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features_post_opt\n",
    "\n",
    "# df_bulk_dft.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_bulk_dft_not_processed.shape: (0, 15)\n"
     ]
    }
   ],
   "source": [
    "ids_to_process = [i for i in df_bulk_dft.index if i not in df_features_post_opt.index]\n",
    "df_bulk_dft_not_processed = df_bulk_dft.loc[ids_to_process]\n",
    "\n",
    "print(\"df_bulk_dft_not_processed.shape:\", df_bulk_dft_not_processed.shape)\n",
    "# df_bulk_dft_not_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_bulk_dft_not_processed.shape[0] == 0:\n",
    "    df_features_post_opt_new = pd.DataFrame()\n",
    "else:\n",
    "    FP_struct = FingerPrint(**{\n",
    "        \"feature_methods\": [\"voronoi\"],\n",
    "        \"input_data\": df_bulk_dft_not_processed,\n",
    "        \"input_index\": [\"atoms\"]})\n",
    "\n",
    "    FP_struct.generate_fingerprints()\n",
    "    df_features_post_opt_new = FP_struct.fingerprints\n",
    "\n",
    "    # Add the 'source' column to features dataframe since there are duplicate ids\n",
    "    # due to the fact that Chris and I ran the same structures\n",
    "    df_features_post_opt_new[\"data\", \"source\"] = df_bulk_dft_not_processed[\"source\"]\n",
    "    df_features_post_opt_new[\"data\", \"INDEX_OLD\"] = df_bulk_dft_not_processed[\"INDEX_OLD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `df_features_post_opt` have any NaN values in it:  \n",
      " --> False\n"
     ]
    }
   ],
   "source": [
    "df_features_post_opt_comb = pd.concat([\n",
    "    df_features_post_opt,\n",
    "    df_features_post_opt_new])\n",
    "\n",
    "nan_mask = df_features_post_opt_comb[\"voronoi\"].isnull().any(axis=\"columns\")\n",
    "\n",
    "df_features_post_opt_comb_cpy = copy.deepcopy(df_features_post_opt_comb)\n",
    "\n",
    "df_features_post_opt_comb = df_features_post_opt_comb.loc[~nan_mask]\n",
    "df_nan_in_voro = df_features_post_opt_comb_cpy.loc[nan_mask]\n",
    "\n",
    "print(\n",
    "    \"Does `df_features_post_opt` have any NaN values in it: \",\n",
    "    \"\\n -->\",\n",
    "    df_features_post_opt_comb[\"voronoi\"].isnull().any(axis=\"columns\").any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_indices = df_features_post_opt_comb[\"data\", \"INDEX_OLD\"]\n",
    "\n",
    "df_features_post_opt_comb = df_features_post_opt_comb.set_index(old_indices)\n",
    "# df_features_post_opt_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_renamed = df_features_post_opt_comb.index.rename(\"id_unique\")\n",
    "df_features_post_opt_comb = df_features_post_opt_comb.set_index(index_renamed)\n",
    "df_features_post_opt_comb = df_features_post_opt_comb.drop((\"data\", \"INDEX_OLD\"), axis=1)\n",
    "\n",
    "index_renamed = df_bulk_dft.index.rename(\"id_unique\")\n",
    "df_bulk_dft = df_bulk_dft.set_index(index_renamed)\n",
    "df_bulk_dft = df_bulk_dft.drop((\"INDEX_OLD\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "with open(df_features_post_opt_path, \"wb\") as fle:\n",
    "    pickle.dump(df_features_post_opt_comb, fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7808a1103591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"# # \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_bulk_dft[df_bulk_dft[\"stoich\"] == \"AB2\"].sort_values(\"energy_pa\")\n",
    "\n",
    "# # df_features_pre_opt\n",
    "\n",
    "# # df_features_pre_opt.shape\n",
    "\n",
    "# # df_features_pre_opt.loc[\"6dzhcimdxs\"]\n",
    "\n",
    "# \"6dzhcimdxs\" in df_bulk_dft.index\n",
    "\n",
    "# assert False\n",
    "\n",
    "# df_struct.index.unique()\n",
    "\n",
    "\n",
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FP_struct = FingerPrint(**{\n",
    "#     \"feature_methods\": [\"voronoi\"],\n",
    "#     \"input_data\": df_struct_kirsten,\n",
    "# #     \"input_data\": df_combined,\n",
    "#     \"input_index\": [\"atoms\"]})\n",
    "\n",
    "# FP_struct.generate_fingerprints()\n",
    "# df_features_pre_opt = FP_struct.fingerprints\n",
    "\n",
    "# # #############################################################################\n",
    "# with open(df_features_pre_opt_kirsten_path, \"wb\") as fle:\n",
    "#     pickle.dump(df_features_pre_opt, fle)\n",
    "\n",
    "# with open(df_features_pre_opt_kirsten_path, \"rb\") as fle:\n",
    "#     df_features_pre_opt_kirsten = pickle.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_features_pre_opt\n",
    "\n",
    "# df_features_pre_opt__before.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"workflow/ml_modelling\"))\n",
    "# from ml_methods import get_ml_dataframes\n",
    "# ml_data_dict = get_ml_dataframes(names=[\"df_features_pre_opt_path\"])\n",
    "# df_features_pre_opt = ml_data_dict[\"df_features_pre_opt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TEMP\n",
    "# df_struct = df_struct.sample(n=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# with open(df_features_pre_opt_path, \"rb\") as fle:\n",
    "#     df_features_pre_opt = pickle.load(fle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_2] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
