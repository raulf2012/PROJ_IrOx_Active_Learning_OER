{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/00_ml_workflow\"))\n",
    "    # \"workflow/ml_modelling/00_ml_workflow/191102_new_workflow\"))\n",
    "from al_data import al_data_files_dict, main_AB2_run, main_AB3_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_AB2_run\n",
    "main_AB3_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Input"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab2_file_list = [\n",
    "    main_AB2_run,\n",
    "    ]\n",
    "\n",
    "ab3_file_list = [\n",
    "    main_AB3_run,\n",
    "    ]\n",
    "\n",
    "file_list_dict = dict(\n",
    "    AB2=ab2_file_list,\n",
    "    AB3=ab3_file_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "def get_duplicates_list(stoich_i, file_list_dict=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    file_list = file_list_dict[stoich_i]\n",
    "\n",
    "    duplicates_lists = []\n",
    "    for file in file_list:\n",
    "        path_i = file\n",
    "\n",
    "        print(path_i)\n",
    "\n",
    "        with open(path_i, \"rb\") as fle:\n",
    "            AL = pickle.load(fle)\n",
    "\n",
    "\n",
    "        last_gen = list(AL.al_gen_dict.keys())[-1]\n",
    "        AL_i = AL.al_gen_dict[last_gen]\n",
    "\n",
    "        model = AL_i.model\n",
    "        model.sort_values(\"y_real\")\n",
    "\n",
    "        duplicates_i = model[model.duplicate == True].index.tolist()\n",
    "        # print(len(duplicates_i))\n",
    "\n",
    "        duplicates_lists.append(duplicates_i)\n",
    "\n",
    "    # #########################################################################\n",
    "    # Checking that all duplicates lists are the same #########################\n",
    "    duplicates_are_the_same_list = []\n",
    "    for duplicates_i in duplicates_lists:\n",
    "        for duplicates_j in duplicates_lists:\n",
    "            duplicates_are_the_same = duplicates_j == duplicates_i\n",
    "            duplicates_are_the_same_list.append(duplicates_are_the_same)\n",
    "    duplicates_are_the_same_final = all(duplicates_are_the_same_list)\n",
    "    assert duplicates_are_the_same_final, \"IJDSFIISD\"\n",
    "\n",
    "\n",
    "    return(duplicates_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_ab2 = get_duplicates_list(\"AB2\", file_list_dict=file_list_dict)\n",
    "duplicates_ab3 = get_duplicates_list(\"AB3\", file_list_dict=file_list_dict)\n",
    "\n",
    "duplicates_dict = dict(\n",
    "    AB2=duplicates_ab2,\n",
    "    AB3=duplicates_ab3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"6fcdbh9fz2\" in duplicates_dict[\"AB3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickling data ######################################################\n",
    "# directory = \"out_data\"\n",
    "# if not os.path.exists(directory): os.makedirs(directory)\n",
    "# with open(os.path.join(directory, \"duplicates.pickle\"), \"wb\") as fle:\n",
    "#     pickle.dump(duplicates_dict, fle)\n",
    "# # #####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Duplicates Manually (Without AL Run)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling\"))\n",
    "from ml_methods import get_ml_dataframes\n",
    "from ccf_similarity.ccf import CCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_dict = get_ml_dataframes()\n",
    "\n",
    "df_dft = DF_dict.get(\"bulk_dft_data\")\n",
    "df_dij = DF_dict.get(\"df_dij\")\n",
    "ids_to_discard__too_many_atoms = DF_dict.get(\"ids_to_discard__too_many_atoms\")\n",
    "\n",
    "\n",
    "df_dft = df_dft[df_dft.source == \"raul\"]\n",
    "df_dft = df_dft.drop(columns=[\"path\", \"id_old\", \"id\", \"form_e_chris\", \"atoms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_i = \"cubqbpzd7k\"\n",
    "id_i in df_dft.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_discard__too_many_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_to_discard__too_many_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dft.shape)\n",
    "df_dft = df_dft.drop(\n",
    "    index=df_dft.index.intersection(ids_to_discard__too_many_atoms)\n",
    "    )\n",
    "print(df_dft.shape)\n",
    "\n",
    "df_dij = df_dij.drop(\n",
    "    index=df_dij.index.intersection(ids_to_discard__too_many_atoms)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicates_list_manually(\n",
    "    stoich_i=None, \n",
    "    df_dft=None,\n",
    "    df_dij=None,\n",
    "    ):\n",
    "    # #########################################################\n",
    "\n",
    "    df_dft = df_dft[df_dft.stoich == stoich_i]\n",
    "\n",
    "    shared_index = df_dft.index.intersection(df_dij.index)\n",
    "    df_dij = df_dij.loc[shared_index, shared_index]\n",
    "\n",
    "    # df_dft = df_dft[df_dft.stoich == stoich_i]\n",
    "    # df_dij = df_dij.loc[df_dft.index, df_dft.index]\n",
    "\n",
    "    CCF_i = CCF(df_dij=df_dij, d_thresh=0.02)\n",
    "\n",
    "\n",
    "    ids_to_drop = []\n",
    "    for id_i in df_dft.index.tolist():\n",
    "        simil_dict_i = CCF_i.i_all_similar(id_i)\n",
    "        if simil_dict_i is not None:\n",
    "            similar_ids = [id_i] + list(simil_dict_i.keys())\n",
    "            df_i = df_dft.loc[similar_ids]\n",
    "            ids_to_drop_i = df_i.sort_values(\"energy_pa\").iloc[1:].index.tolist()\n",
    "            ids_to_drop.extend(ids_to_drop_i)\n",
    "\n",
    "    ids_to_drop__duplicates = ids_to_drop\n",
    "    ids_to_drop__duplicates = list(set(ids_to_drop__duplicates))\n",
    "    \n",
    "    return(ids_to_drop__duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_dft = df_dft[df_dft.source == \"raul\"]\n",
    "\n",
    "# stoich_i = \"AB3\"\n",
    "\n",
    "# df_dft = df_dft[df_dft.stoich == stoich_i]\n",
    "\n",
    "# shared_index = df_dft.index.intersection(df_dij.index)\n",
    "\n",
    "# df_dij = df_dij.loc[shared_index, shared_index]\n",
    "\n",
    "# # df_dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_ab2_manual = get_duplicates_list_manually(stoich_i=\"AB2\", df_dft=df_dft, df_dij=df_dij)\n",
    "\n",
    "duplicates_ab3_manual = get_duplicates_list_manually(stoich_i=\"AB3\", df_dft=df_dft, df_dij=df_dij)\n",
    "\n",
    "duplicates_dict_manual = dict(\n",
    "    AB2=duplicates_ab2_manual,\n",
    "    AB3=duplicates_ab3_manual,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ######################################################\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"duplicates.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(duplicates_dict_manual, fle)\n",
    "# #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing duplicate lists constructed from AL to those constructed manually"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(duplicates_ab3_manual))\n",
    "print(len(set(duplicates_ab3)))\n",
    "\n",
    "print(\"\")\n",
    "for i in duplicates_ab3_manual:\n",
    "    if i not in duplicates_ab3:\n",
    "        print(i)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i in duplicates_ab3:\n",
    "    if i not in duplicates_ab3_manual:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           duplicates_ab2_manual\n",
    "print(len(duplicates_ab2_manual))\n",
    "print(len(set(duplicates_ab2)))\n",
    "\n",
    "print(\"\")\n",
    "for i in duplicates_ab2_manual:\n",
    "    if i not in duplicates_ab2:\n",
    "        print(i)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i in duplicates_ab2:\n",
    "    if i not in duplicates_ab2_manual:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that it is best to go with the duplicates processed manually\n",
    "\n",
    "It looks like the AL runs are missing something"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoich_i = \"AB2\"\n",
    "\n",
    "# stoich_i=None,\n",
    "df_dft=df_dft\n",
    "df_dij=df_dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_duplicates_list_manually(\n",
    "# stoich_i=None, \n",
    "# df_dft=None,\n",
    "# df_dij=None,\n",
    "# ):\n",
    "\n",
    "# #########################################################\n",
    "# df_dft = df_dft[df_dft.source == \"raul\"]\n",
    "df_dft = df_dft[df_dft.stoich == stoich_i]\n",
    "\n",
    "df_dij = df_dij.loc[df_dft.index, df_dft.index]\n",
    "\n",
    "\n",
    "CCF_i = CCF(df_dij=df_dij, d_thresh=0.02)\n",
    "\n",
    "\n",
    "ids_to_drop = []\n",
    "for id_i in df_dft.index.tolist():\n",
    "    simil_dict_i = CCF_i.i_all_similar(id_i)\n",
    "    if simil_dict_i is not None:\n",
    "        similar_ids = [id_i] + list(simil_dict_i.keys())\n",
    "        df_i = df_dft.loc[similar_ids]\n",
    "        ids_to_drop_i = df_i.sort_values(\"energy_pa\").iloc[1:].index.tolist()\n",
    "        ids_to_drop.extend(ids_to_drop_i)\n",
    "\n",
    "ids_to_drop__duplicates = ids_to_drop\n",
    "ids_to_drop__duplicates = list(set(ids_to_drop__duplicates))\n",
    "\n",
    "# return(ids_to_drop__duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing old and new duplicates"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox_2\"],\n",
    "    \"FIGS_IrOx_Active_Learning_OER/01_figures/00_main_publ_figs/03_E_vs_V_coord/scripts\",\n",
    "    \"old.duplicates.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    duplicates_dict_old = pickle.load(fle)\n",
    "# #############################################################################\n",
    "\n",
    "# #############################################################################\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox_2\"],\n",
    "    \"FIGS_IrOx_Active_Learning_OER/01_figures/00_main_publ_figs/03_E_vs_V_coord/scripts\",\n",
    "    \"new.duplicates.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    duplicates_dict_new = pickle.load(fle)\n",
    "# #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "duplicates_dict_new[\"AB2\"] == duplicates_dict_old[\"AB2\"]\n",
    "\n",
    "print(len(duplicates_dict_new[\"AB2\"]))\n",
    "print(len(duplicates_dict_old[\"AB2\"]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
