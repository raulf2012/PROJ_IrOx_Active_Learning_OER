{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New ML Active Learning Workflow\n",
    "---\n",
    "\n",
    "32 (Too big, not computed),\n",
    "\n",
    "226 (Not computed),"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#| - OUT_OF_SIGHT\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "t0_init = time.time()\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ase.visualize import view\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path, unique_ids_path,\n",
    "    df_features_pre_opt_path,\n",
    "    df_features_pre_opt_kirsten_path,\n",
    "    df_features_post_opt_path,\n",
    "    ids_to_discard__too_many_atoms_path,\n",
    "    )\n",
    "\n",
    "from gp_methods import gp_model_gpflow, gp_model_catlearn\n",
    "\n",
    "from methods import get_trace_j\n",
    "from plotting.my_plotly import my_plotly_plot\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "from gp_methods import gp_workflow, job_aquisition, test_al_conv\n",
    "\n",
    "sys.path.insert(0,\n",
    "    os.path.join(os.environ[\"PROJ_irox\"], \"workflow/ml_modelling\"))\n",
    "\n",
    "from ml_methods import create_mixed_df\n",
    "\n",
    "from ase_modules.ase_methods import view_in_vesta\n",
    "\n",
    "sys.path.insert(0, \"../04_final_ml_plots\")\n",
    "from layout import get_layout\n",
    "\n",
    "layout = get_layout(model=None)\n",
    "\n",
    "from methods import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gp_methods import random_job_aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoich_i = \"AB3\"\n",
    "# stoich_i = \"AB2\"\n",
    "\n",
    "custom_name = \"regular\"\n",
    "# custom_name = \"random\"\n",
    "\n",
    "# gp_model = gp_model_gpflow\n",
    "gp_model = gp_model_catlearn\n",
    "\n",
    "# aqs_bin_size = 5\n",
    "aqs_bin_size = 10\n",
    "\n",
    "# output_key = \"form_e_chris\"\n",
    "output_key = \"energy_pa\"\n",
    "\n",
    "verbosity_level = 6  # 1-10 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "\n",
    "    # \"noise\": [0.02542],\n",
    "    # \"sigma_l\": [0.0049],\n",
    "    # \"sigma_f\": [5.19],\n",
    "    # \"alpha\": [0.018],\n",
    "\n",
    "    # \"noise\": [0.000001],\n",
    "    # \"sigma_l\": [0.001],\n",
    "    # \"sigma_f\": [0.5],\n",
    "    # \"alpha\": [0.5],\n",
    "\n",
    "    # Good for AB3\n",
    "    \"noise\": [0.02542],\n",
    "    \"sigma_l\": [1.0049],\n",
    "    \"sigma_f\": [5.19],\n",
    "    \"alpha\": [0.018],\n",
    "\n",
    "    # \"noise\": [0.02542],\n",
    "    # \"sigma_l\": [10.0049],\n",
    "    # \"sigma_f\": [5.19],\n",
    "    # \"alpha\": [0.018],\n",
    "\n",
    "    }\n",
    "\n",
    "# noise = 0.0042  # Regularisation parameter.\n",
    "# sigma_l = 6.3917  # Length scale parameter.\n",
    "# sigma_f = 0.5120  # Scaling parameter.\n",
    "# alpha = 0.3907  # Alpha parameter.\n",
    "\n",
    "\n",
    "c = list(itertools.product(*params_dict.values()))\n",
    "df_gp_params = pd.DataFrame(c, columns=params_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_i = \"out_data\";\n",
    "if not os.path.exists(dir_i):\n",
    "    os.makedirs(dir_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "    df_bulk_dft = pickle.load(fle)\n",
    "\n",
    "with open(df_features_pre_opt_path, \"rb\") as fle:\n",
    "    df_features_pre = pickle.load(fle)\n",
    "\n",
    "# with open(df_features_pre_opt_kirsten_path, \"rb\") as fle:\n",
    "#     df_features_pre = pickle.load(fle)\n",
    "\n",
    "with open(df_features_post_opt_path, \"rb\") as fle:\n",
    "    df_features_post = pickle.load(fle)\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)\n",
    "\n",
    "df_ids = df_ids[\n",
    "    (df_ids[\"stoich\"] == stoich_i) & \\\n",
    "    (df_ids[\"source\"] != \"oqmd\") & \\\n",
    "    (df_ids[\"source\"] != \"raul_oer\") & \\\n",
    "    [True for i in range(len(df_ids))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering dataframes to the correct stoicheometry"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP DROP DUPLICATE and OUTLIER SYSTEMS"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect ids to ignore list"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids[df_ids[\"unique_ids\"] == 'zwmivrzazu']\n",
    "\n",
    "# 'zwmivrzazu' in df_ids\n",
    "\n",
    "# df_ids[\"unique_ids\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_ids.index.unique().tolist()\n",
    "\n",
    "# print(\"len(ids_to_drop__duplicates):\", len(ids_to_drop__duplicates))\n",
    "# ids_to_drop__duplicates = [i for i in ids_to_drop__duplicates if i in df_ids[\"unique_ids\"].unique().tolist()]\n",
    "# print(\"len(ids_to_drop__duplicates):\", len(ids_to_drop__duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/ccf_similarity_analysis/out_data\",\n",
    "    \"all_ids_to_elim_1.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    ids_to_drop__duplicates = pickle.load(fle)\n",
    "    ids_to_drop__duplicates = ids_to_drop__duplicates[stoich_i]\n",
    "\n",
    "    print(\"len(ids_to_drop__duplicates):\", len(ids_to_drop__duplicates))\n",
    "    ids_to_drop__duplicates = [i for i in ids_to_drop__duplicates if i in df_ids[\"unique_ids\"].unique().tolist()]\n",
    "    print(\"len(ids_to_drop__duplicates):\", len(ids_to_drop__duplicates))\n",
    "\n",
    "# #############################################################################\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/visualizing_data/out_data\",\n",
    "    \"outlier_features.json\")\n",
    "with open(path_i, 'r') as f:\n",
    "    ids_to_drop__outliers = json.load(f)\n",
    "\n",
    "    print(\"len(ids_to_drop__outliers):\", len(ids_to_drop__outliers))\n",
    "    ids_to_drop__outliers = [i for i in ids_to_drop__outliers if i in df_ids[\"unique_ids\"].unique().tolist()]\n",
    "    print(\"len(ids_to_drop__outliers):\", len(ids_to_drop__outliers))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "with open(ids_to_discard__too_many_atoms_path, \"rb\") as fle:\n",
    "    ids_to_drop__too_many_atoms = pickle.load(fle)\n",
    "\n",
    "    print(\"len(ids_to_drop__too_many_atoms):\", len(ids_to_drop__too_many_atoms))\n",
    "    ids_to_drop__too_many_atoms = [i for i in ids_to_drop__too_many_atoms if i in df_ids[\"unique_ids\"].unique().tolist()]\n",
    "    print(\"len(ids_to_drop__too_many_atoms):\", len(ids_to_drop__too_many_atoms))\n",
    "\n",
    "# #############################################################################\n",
    "ids_to_drop = [] + \\\n",
    "    ids_to_drop__too_many_atoms\n",
    "    # ids_to_drop__duplicates + \\\n",
    "    # ids_to_drop__outliers + \\\n",
    "\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))\n",
    "ids_to_drop = list(set(ids_to_drop))\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))\n",
    "\n",
    "abx_ids = df_ids[df_ids[\"stoich\"] == stoich_i][\"unique_ids\"].tolist()\n",
    "ids_to_drop = [i for i in ids_to_drop if i in abx_ids]\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Filter ids ##################################################################\n",
    "# df_ids = df_ids[\n",
    "#     (df_ids[\"stoich\"] == stoich_i) & \\\n",
    "#     (df_ids[\"source\"] != \"oqmd\") & \\\n",
    "#     (df_ids[\"source\"] != \"raul_oer\") & \\\n",
    "#     [True for i in range(len(df_ids))]\n",
    "#     ]\n",
    "\n",
    "print(\"df_ids.shape:\", df_ids.shape)\n",
    "# IDS TO DROP\n",
    "df_ids = df_ids[~df_ids[\"unique_ids\"].isin(ids_to_drop)]\n",
    "print(\"df_ids.shape:\", df_ids.shape)\n",
    "unique_ids = df_ids[\"unique_ids\"].tolist()\n",
    "\n",
    "# #############################################################################\n",
    "# Training Features ###########################################################\n",
    "index_filter = np.intersect1d(df_features_post.index, unique_ids)\n",
    "df_features_post = df_features_post.loc[index_filter]\n",
    "\n",
    "# #############################################################################\n",
    "# Training Features ###########################################################\n",
    "index_filter = np.intersect1d(df_bulk_dft.index, unique_ids)\n",
    "print(\"df_bulk_dft.shape:\", df_bulk_dft.shape)\n",
    "df_bulk_dft = df_bulk_dft.loc[index_filter]\n",
    "print(\"df_bulk_dft.shape:\", df_bulk_dft.shape)\n",
    "\n",
    "# #############################################################################\n",
    "# Test Features ###############################################################\n",
    "index_filter = np.intersect1d(df_features_pre.index, unique_ids)\n",
    "df_features_pre = df_features_pre.loc[index_filter]\n",
    "\n",
    "# #############################################################################\n",
    "# Filter training data ########################################################\n",
    "df_features_post = \\\n",
    "    df_features_post[df_features_post[\"data\"][\"source\"] != \"chris\"]\n",
    "df_bulk_dft = df_bulk_dft[df_bulk_dft[\"source\"] != \"chris\"]\n",
    "\n",
    "# #############################################################################\n",
    "df_post = df_features_post[\"voronoi\"]\n",
    "df_pre = df_features_pre[\"voronoi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get initial computed ids randomly"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = df_features_pre.index.unique()\n",
    "print(\"len(all_ids):\", len(all_ids))\n",
    "\n",
    "computed_ids = df_bulk_dft.index.unique()\n",
    "computed_ids = np.random.choice(computed_ids, size=20)\n",
    "computed_ids = list(set(computed_ids))[0:11]\n",
    "print(\"len(computed_ids):\", len(computed_ids))\n",
    "\n",
    "# TEMP | Use all training data initially **************************************\n",
    "computed_ids = df_bulk_dft.index.tolist()\n",
    "#__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running GP models"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layout[\"xaxis\"][\"range\"] = None\n",
    "# layout[\"yaxis\"][\"range\"] = None\n",
    "# layout[\"showlegend\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features_post=df_train, df_test=df_test_tmp,\n",
    "# df_bulk_dft=df_bulk_dft_i, df_bulk_dft_all=df_bulk_dft,\n",
    "# df_ids=df_ids, gp_model=gp_model_catlearn,\n",
    "# opt_hyperparameters=True, gp_params=gp_params_i,\n",
    "# y_train_key=\"energy_pa\", verbose=False, pca_comp=11,\n",
    "# pca_mode=\"num_comp\")\n",
    "\n",
    "# df_train\n",
    "# df_test_tmp\n",
    "# df_bulk_dft_i\n",
    "# df_bulk_dft\n",
    "# df_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bulk_dft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "for al_iter_i in range(80):\n",
    "# for al_iter_i in range(5):\n",
    "    #| - GP AL Iteration ******************************************************\n",
    "    # *************************************************************************\n",
    "    t0 = time.time(); num_training = str(len(computed_ids)).zfill(3)\n",
    "    al_iter_i_str = str(al_iter_i).zfill(3)\n",
    "    print(al_iter_i_str, \" | \", num_training + \" \" + 68 * \"#\"); print(80 * \"#\")\n",
    "    row_i = df_gp_params.iloc[0]\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # #########################################################################\n",
    "    df_test_tmp = create_mixed_df(\n",
    "        all_ids, computed_ids,\n",
    "        df_post, df_pre, verbose=False)\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Filter training data by 'computed' ids ##################################\n",
    "    computed_ids = list(set(computed_ids))\n",
    "    df_bulk_dft_i = df_bulk_dft.loc[computed_ids]\n",
    "    df_train = df_post.loc[computed_ids]\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Running GP Model ########################################################\n",
    "    gp_params_i = row_i.to_dict()\n",
    "    out = gp_workflow(\n",
    "        df_features_post=df_train, df_test=df_test_tmp,\n",
    "        df_bulk_dft=df_bulk_dft_i, df_bulk_dft_all=df_bulk_dft,\n",
    "        df_ids=df_ids, gp_model=gp_model_catlearn,\n",
    "        opt_hyperparameters=True, gp_params=gp_params_i,\n",
    "        y_train_key=\"energy_pa\", verbose=True, pca_comp=11,\n",
    "        pca_mode=\"num_comp\")\n",
    "\n",
    "    model_i = out[\"model\"]; model_inst = out[\"model_inst\"]\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Job Aquisition ##########################################################    \n",
    "    if custom_name == \"random\":\n",
    "        aquisition_out_dict = random_job_aquisition(\n",
    "            model_i, aqs_bin_size=aqs_bin_size,\n",
    "            df_bulk_dft_all=df_bulk_dft,\n",
    "            y_train_key=\"energy_pa\")\n",
    "    else:\n",
    "        aquisition_out_dict = job_aquisition(\n",
    "            model_i, aqs_bin_size=aqs_bin_size,\n",
    "            df_bulk_dft_all=df_bulk_dft,\n",
    "            y_train_key=\"energy_pa\")\n",
    "\n",
    "    new_ids_to_compute = aquisition_out_dict[\"new_ids_to_compute\"]\n",
    "    computed_ids += new_ids_to_compute\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Test for AL Convergence #################################################\n",
    "    al_converged = test_al_conv(model_i)\n",
    "    if al_converged:\n",
    "        print(\"AL CONVERGED!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # SAVE DATA ###############################################################\n",
    "    out_dict = {**out,\n",
    "        \"gp_instance\": model_inst, \"computed_ids\": computed_ids,\n",
    "        \"aquisition_data\": aquisition_out_dict, \"al_converged\": al_converged}\n",
    "    data_dict[al_iter_i] = out_dict\n",
    "\n",
    "\n",
    "    file_name_i = \"data_dict_\" + stoich_i + \"_\" + custom_name + \".pickle\"\n",
    "    with open(os.path.join(dir_i, file_name_i), \"wb\") as fle:\n",
    "        pickle.dump(data_dict, fle)\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # BREAKING LOOP WHEN DFT DATA RUNS OUT ####################################\n",
    "    if len(new_ids_to_compute) == 0:\n",
    "        print(\"NO MORE DFT DATA AVAILABLE\")\n",
    "        break\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Printing loop time info #################################################\n",
    "    if verbosity_level > 5:\n",
    "        print(\"Loop time (sec):\", (time.time() - t0), \"\\n\");\n",
    "\n",
    "    #__| **********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################################\n",
    "# CREATE FIGURE ###########################################################\n",
    "# COMBAK | Don't need to create the figure in this step | TODO\n",
    "# fig_i = plot_model(\n",
    "#     model_i, layout=get_layout(model_i), model_i=model_i,\n",
    "#     df_bulk_dft=df_bulk_dft, name=al_iter_i_str, custom_text=num_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.shape\n",
    "\n",
    "df_bulk_dft_i.shape\n",
    "df_train.shape\n",
    "\n",
    "# computed_ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID's that were needed but are not computed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_needed_all = []\n",
    "for key, val in data_dict.items():\n",
    "    ids_needed = val[\"aquisition_data\"][\"ids_needed_but_not_avail\"]\n",
    "#     ids_needed = val[\"ids_needed_but_not_avail\"]\n",
    "    if not val[\"al_converged\"]:\n",
    "        ids_needed_all += ids_needed\n",
    "\n",
    "print(\"IDs that were needed during AL but were not computed\",\n",
    "    \"\\n\", list(set(ids_needed_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting last AL generation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_i = data_dict[\n",
    "#     list(data_dict.keys())[-1]]\n",
    "\n",
    "# gen_i[\"fig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Log Marginal Likelihood for each model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lml_list = []\n",
    "for key, val in data_dict.items():\n",
    "    model = val[\"model\"]\n",
    "    gp_instance = val[\"gp_instance\"]\n",
    "\n",
    "    # #########################################################################\n",
    "    gp_instance.kernel_list\n",
    "\n",
    "    # if type(gp_instance.log_marginal_likelihood) == list:\n",
    "    try:\n",
    "        lml_i = gp_instance.log_marginal_likelihood[0]\n",
    "    except:\n",
    "    # else:\n",
    "        lml_i = gp_instance.log_marginal_likelihood\n",
    "    lml_list.append(lml_i)\n",
    "\n",
    "\n",
    "trace = go.Scatter(\n",
    "    y=lml_list,\n",
    "    mode=\"markers\")\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook executed in \", time.time() - t0_init, \"(s)\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################################\n",
    "# #########################################################################\n",
    "computed_ids = []\n",
    "df_test_tmp = create_mixed_df(\n",
    "    all_ids, computed_ids,\n",
    "    df_post, df_pre, verbose=False)\n",
    "\n",
    "# #########################################################################\n",
    "# Filter training data by 'computed' ids ##################################\n",
    "# computed_ids = list(set(computed_ids))\n",
    "\n",
    "# df_bulk_dft_i = df_bulk_dft.loc[computed_ids]\n",
    "df_bulk_dft_i = df_bulk_dft\n",
    "\n",
    "# df_train = df_post.loc[computed_ids]\n",
    "df_train = df_post\n",
    "\n",
    "\n",
    "intersected_ids = list(set(df_bulk_dft_i.index) & set(df_train.index))\n",
    "df_train = df_train.loc[intersected_ids]\n",
    "df_bulk_dft_i = df_bulk_dft_i.loc[intersected_ids]\n",
    "\n",
    "# #########################################################################\n",
    "# Running GP Model ########################################################\n",
    "gp_params_i = row_i.to_dict()\n",
    "out = gp_workflow(\n",
    "    df_features_post=df_train, df_test=df_test_tmp,\n",
    "    df_bulk_dft=df_bulk_dft_i, df_bulk_dft_all=df_bulk_dft,\n",
    "    df_ids=df_ids, gp_model=gp_model_catlearn,\n",
    "    opt_hyperparameters=True, gp_params=gp_params_i,\n",
    "    y_train_key=\"energy_pa\", verbose=False, pca_comp=11,\n",
    "    pca_mode=\"num_comp\")\n",
    "\n",
    "model_i = out[\"model\"]; model_inst = out[\"model_inst\"]\n",
    "\n",
    "model_i = model_i[~model_i[\"energy_pa\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (abs(model_i[\"prediction_unstandardized\"] - model_i[\"energy_pa\"]) ** 2)\n",
    "y_pred = model_i[\"prediction_unstandardized\"]\n",
    "y_test = model_i[\"energy_pa\"]\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rms = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(rms)\n",
    "\n",
    "# 0.5370373324014677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import os\n",
    "\n",
    "# x_array = [0, 1, 2, 3]\n",
    "# y_array = [0, 1, 2, 3]\n",
    "\n",
    "x_array = model_i[\"prediction_unstandardized\"]\n",
    "y_array = model_i[\"energy_pa\"]\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=x_array,\n",
    "    y=y_array,\n",
    "    mode=\"markers\",\n",
    "\n",
    "    marker=dict(\n",
    "        symbol=\"circle\",\n",
    "        color='LightSkyBlue',\n",
    "        size=14,\n",
    "        line=dict(\n",
    "            color='MediumPurple',\n",
    "            width=2\n",
    "            )\n",
    "        ),\n",
    "\n",
    "    line=dict(\n",
    "        color=\"firebrick\",\n",
    "        width=2,\n",
    "        dash=\"dot\",\n",
    "        ),\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "trace_0 = go.Scatter(\n",
    "    x=[-7, -4],\n",
    "    y=[-7, -4],\n",
    "    mode=\"lines\",\n",
    "\n",
    "#     marker=dict(\n",
    "#         symbol=\"circle\",\n",
    "#         color='LightSkyBlue',\n",
    "#         size=14,\n",
    "#         line=dict(\n",
    "#             color='MediumPurple',\n",
    "#             width=2\n",
    "#             )\n",
    "#         ),\n",
    "\n",
    "    line=dict(\n",
    "        color=\"firebrick\",\n",
    "        width=2,\n",
    "        dash=\"dot\",\n",
    "        ),\n",
    "\n",
    "    )\n",
    "data = [trace, trace_0]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:research-new]",
   "language": "python",
   "name": "conda-env-research-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
