{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gpflow\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# #############################################################################\n",
    "from catlearn.regression.gaussian_process import GaussianProcess\n",
    "from catlearn.preprocess.clean_data import (\n",
    "    clean_infinite,\n",
    "    clean_variance,\n",
    "    clean_skewness)\n",
    "from catlearn.preprocess.scaling import standardize\n",
    "\n",
    "# #############################################################################\n",
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"python_classes/active_learning\"))\n",
    "from active_learning import (\n",
    "    ALBulkOpt,\n",
    "    ALGeneration,\n",
    "    RegressionModel,\n",
    "    FingerPrints,\n",
    "    CandidateSpace,\n",
    "    )\n",
    "\n",
    "from al_analysis import ALAnalysis, ALAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"python_classes\"))\n",
    "from ccf_similarity.ccf import CCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pathos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-500c02ce16ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpathos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessingPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pathos'"
     ]
    }
   ],
   "source": [
    "from pathos.multiprocessing import ProcessingPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool\n",
    "class Bar:\n",
    "    def foo(self, name):\n",
    "        return(len(str(name)))\n",
    "    def boo(self, things):\n",
    "        for thing in things:\n",
    "            self.sum += self.foo(thing)\n",
    "        return(self.sum)\n",
    "    sum = 0\n",
    "\n",
    "b = Bar()\n",
    "results = ProcessingPool().map(b.boo, [[12,3,456],[8,9,10],['a','b','cde']])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "import pickle; import os\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/00_ml_workflow\",\n",
    "    \"dev_new_al_class/out_data\",\n",
    "    \"TEST_small.pickle\")\n",
    "    # \"AL_AB2_05.pickle\")\n",
    "    # \"AL_AB2_06.pickle\")\n",
    "\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    AL = pickle.load(fle)\n",
    "\n",
    "# #############################################################################\n",
    "self = AL\n",
    "\n",
    "seed_ids = AL.seed_ids\n",
    "index_acq_gen_dict = AL.index_acq_gen_dict\n",
    "# #############################################################################\n",
    "AL_i = AL.al_gen_dict[2]\n",
    "self = AL_i\n",
    "\n",
    "completed_ids = self.completed_ids\n",
    "CandidateSpace = self.CandidateSpace\n",
    "model = self.model\n",
    "verbose = self.verbose\n",
    "# df_train = self.df_train\n",
    "# df_test = self.df_test\n",
    "verbose = self.verbose\n",
    "acquisition_bin = self.acquisition_bin\n",
    "RegressionModel = self.RegressionModel\n",
    "# #############################################################################\n",
    "\n",
    "\n",
    "model[model[\"acquired\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_i.__run_duplicate_analysis__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = AL_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "acquisition_bin = self.acquisition_bin\n",
    "model = self.model\n",
    "DuplicateFinder = self.DuplicateFinder\n",
    "index_acq_gen_dict = self.index_acq_gen_dict\n",
    "# #####################################################################\n",
    "\n",
    "\n",
    "# #####################################################################\n",
    "# #####################################################################\n",
    "#| - Apply 'gen_acquired' to model df\n",
    "def method(row_i, index_acq_gen_dict):\n",
    "    index_i = row_i.name\n",
    "    gen_i = index_acq_gen_dict.get(index_i, np.nan)\n",
    "    return(gen_i)\n",
    "\n",
    "model[\"gen_acquired\"] = model.apply(\n",
    "    method, axis=1,\n",
    "    args=(index_acq_gen_dict, ))\n",
    "# __|\n",
    "\n",
    "\n",
    "# #####################################################################\n",
    "# #####################################################################\n",
    "# #####################################################################\n",
    "model_acq = model[model[\"acquired\"] == True]\n",
    "\n",
    "# Only consider duplicates in the set of structures that have been computed\n",
    "filter_ids = model_acq.index.tolist()\n",
    "\n",
    "simil_dict_master = dict()\n",
    "for index_i in model_acq.index.tolist():\n",
    "    simil_dict = DuplicateFinder.i_all_similar(\n",
    "        index_i, filter_ids=filter_ids)\n",
    "\n",
    "    simil_dict_master[index_i] = simil_dict\n",
    "\n",
    "keys_to_delete = []\n",
    "for key, val in simil_dict_master.items():\n",
    "    if val == dict() or val is None:\n",
    "        keys_to_delete.append(key)\n",
    "\n",
    "for key in keys_to_delete:\n",
    "    del simil_dict_master[key]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if len(simil_dict_master.keys()) == 0:\n",
    "    self.indices_that_are_duplicates = []\n",
    "\n",
    "else:\n",
    "    # #####################################################################\n",
    "    # #####################################################################\n",
    "    # #####################################################################\n",
    "    keys = list(simil_dict_master.keys())\n",
    "\n",
    "    tmp_list = [np.array(list(i.keys())) for i in simil_dict_master.values()]\n",
    "    all_ids_from_duplicate_analysis = keys + list(np.hstack(tmp_list))\n",
    "    all_ids_from_duplicate_analysis = list(set(all_ids_from_duplicate_analysis))\n",
    "\n",
    "    # #####################################################################\n",
    "    # #####################################################################\n",
    "    # #####################################################################\n",
    "\n",
    "    # Tracks ids that have already been identified as duplicates\n",
    "    # Don't consider further, already being removed/treated\n",
    "    indices_that_are_duplicates = []\n",
    "    for key, val in simil_dict_master.items():\n",
    "        \n",
    "        if key in indices_that_are_duplicates:\n",
    "            continue\n",
    "\n",
    "        ids_of_duplicates = [key] + list(val.keys())\n",
    "\n",
    "        ids_of_duplicates = \\\n",
    "            [i for i in ids_of_duplicates if i not in indices_that_are_duplicates]\n",
    "\n",
    "        # Skip loop if no duplicate ids are present\n",
    "        if len(ids_of_duplicates) <= 1:\n",
    "            continue\n",
    "\n",
    "        df_tmp = model.loc[ids_of_duplicates].sort_values(\"gen_acquired\")\n",
    "        assert df_tmp.shape[0] > 1, \"Only one row in df_tmp\"\n",
    "        self.TEMP__df_tmp = df_tmp\n",
    "\n",
    "        earlist_gen = df_tmp.iloc[0][\"gen_acquired\"]\n",
    "\n",
    "        # Check that there is only 1 row from previous generations\n",
    "        # If this is working, then all duplicates are removed as they occur,\n",
    "        # so there shouldn't be any left overs\n",
    "        earliest_acq_row = df_tmp.iloc[0]\n",
    "        generations_acquired = df_tmp[\"gen_acquired\"].tolist()\n",
    "\n",
    "        if len(list(set(generations_acquired))) == 1:\n",
    "            print(\"All duplicates acquired at the same gen | OK\")\n",
    "        else:\n",
    "            mess = \"There shouldn't be more than one duplicate from previous generations\"\n",
    "            num_early_gens = generations_acquired.count(earliest_acq_row[\"gen_acquired\"])\n",
    "            # assert num_early_gens == 1, mess\n",
    "\n",
    "\n",
    "        # Are there multiple early gen rows to choose from?\n",
    "        # Should only happen if multiple are acquired at once\n",
    "        multiple_early_gens_present = False\n",
    "        if len(list(set(generations_acquired))) == 1:\n",
    "            print(\"multiple_early_gens_present\")\n",
    "            print(\"TEMP\")\n",
    "            multiple_early_gens_present = True\n",
    "            # break\n",
    "\n",
    "\n",
    "        selected_row = \\\n",
    "            df_tmp[df_tmp[\"gen_acquired\"] == earlist_gen].sort_values(\"y_real\").iloc[0]\n",
    "\n",
    "        # | - OLD | Trying to replace value for lowest energy duplicate\n",
    "        # lowest_y_row = df_tmp.sort_values(\"y_real\").iloc[0]\n",
    "        # TEMP\n",
    "        # lowest_y_row = df_tmp.sort_values(\"y_real\").iloc[1]\n",
    "        # if earliest_acq_row.name != lowest_y_row.name:\n",
    "        #     print(earliest_acq_row.name, lowest_y_row.name)\n",
    "        # #     model.loc[lowest_y_row.name]\n",
    "        # #     model.rename(\n",
    "        # #         index={\n",
    "        # #             lowest_y_row.name: earliest_acq_row.name + \"_TEMP\",\n",
    "        # #             earliest_acq_row.name: lowest_y_row.name,\n",
    "        # #             }, inplace=True)\n",
    "        # #     model.rename(\n",
    "        # #         index={\n",
    "        # #             earliest_acq_row.name + \"_TEMP\": earliest_acq_row.name,\n",
    "        # #             }, inplace=True)\n",
    "        # __|\n",
    "\n",
    "        indices_that_are_duplicates_i = df_tmp.index.tolist()\n",
    "        indices_that_are_duplicates_i.remove(selected_row.name)\n",
    "\n",
    "        indices_that_are_duplicates.extend(indices_that_are_duplicates_i)\n",
    "\n",
    "\n",
    "    indices_that_are_duplicates = list(set(indices_that_are_duplicates))\n",
    "    self.indices_that_are_duplicates = indices_that_are_duplicates\n",
    "\n",
    "    [i for i in all_ids_from_duplicate_analysis if i not in indices_that_are_duplicates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_duplicates\n",
    "\n",
    "# generations_acquired\n",
    "\n",
    "ids_of_duplicates = [key] + list(val.keys())\n",
    "\n",
    "# ids_of_duplicates = \\\n",
    "#     [i for i in ids_of_duplicates if i not in indices_that_are_duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.keys()\n",
    "\n",
    "print(key)\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_that_are_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, AL_i in AL.al_gen_dict.items():\n",
    "#     duplicates = AL_i.indices_that_are_duplicates\n",
    "\n",
    "# model[\"duplicates\"] = [True if i in duplicates else False for i in model.index.tolist()]\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling\"))\n",
    "from ml_methods import get_data_for_al, get_ml_dataframes\n",
    "\n",
    "out_dict = get_data_for_al(\n",
    "    stoich=\"AB2\",\n",
    "    verbose=False,\n",
    "    drop_too_many_atoms=True)\n",
    "\n",
    "# df_features_pre = out_dict[\"df_features_pre\"]\n",
    "df_static_irox = out_dict[\"df_static_irox\"]\n",
    "# df_bulk_dft = out_dict[\"df_bulk_dft\"]\n",
    "df_dij = out_dict[\"df_dij\"]\n",
    "\n",
    "all_indices = df_static_irox.index\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"python_classes\"))\n",
    "from ccf_similarity.ccf import CCF\n",
    "\n",
    "d_thresh = 0.02\n",
    "CCF = CCF(\n",
    "    df_dij=df_dij,\n",
    "    d_thresh=d_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acq = model[model[\"acquired\"] == True]\n",
    "\n",
    "# Only consider duplicates in the set of structures that have been computed\n",
    "filter_ids = model_acq.index.tolist()\n",
    "\n",
    "simil_dict_master = dict()\n",
    "for index_i in model_acq.index.tolist():\n",
    "    simil_dict = CCF.i_all_similar(\n",
    "        index_i, filter_ids=filter_ids)\n",
    "\n",
    "    simil_dict_master[index_i] = simil_dict\n",
    "\n",
    "keys_to_delete = []\n",
    "for key, val in simil_dict_master.items():\n",
    "    if val == dict() or val is None:\n",
    "        keys_to_delete.append(key)\n",
    "\n",
    "for key in keys_to_delete:\n",
    "    del simil_dict_master[key]\n",
    "\n",
    "# simil_dict_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(simil_dict_master.keys())\n",
    "\n",
    "tmp_list = [np.array(list(i.keys())) for i in simil_dict_master.values()]\n",
    "all_ids_from_duplicate_analysis = keys + list(np.hstack(tmp_list))\n",
    "\n",
    "all_ids_from_duplicate_analysis = list(set(all_ids_from_duplicate_analysis))\n",
    "\n",
    "all_ids_from_duplicate_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i, index_acq_gen_dict):\n",
    "    index_i = row_i.name\n",
    "    gen_i = index_acq_gen_dict.get(index_i, np.nan)\n",
    "    return(gen_i)\n",
    "\n",
    "model[\"gen_acquired\"] = model.apply(\n",
    "    method, axis=1,\n",
    "    args=(index_acq_gen_dict, ))\n",
    "\n",
    "# model[~model[\"gen_acquired\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model[model[\"acquired\"] == True].loc[[\n",
    "#     \"xhbabrx4zq\",\n",
    "#     \"zszinjv3zf\",\n",
    "#     \"727lmkmq74\",\n",
    "#     \"zgntxjxrvj\",\n",
    "#     ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_that_are_duplicates = []\n",
    "for key, val in simil_dict_master.items():\n",
    "    ids_of_duplicates = [key] + list(val.keys())\n",
    "\n",
    "    ids_of_duplicates = \\\n",
    "        [i for i in ids_of_duplicates if i not in indices_that_are_duplicates]\n",
    "\n",
    "    # Skip loop if no duplicate ids are present\n",
    "    if len(ids_of_duplicates) == 0:\n",
    "        continue\n",
    "\n",
    "    df_tmp = model.loc[ids_of_duplicates].sort_values(\"gen_acquired\")\n",
    "    earlist_gen = df_tmp.iloc[0][\"gen_acquired\"]\n",
    "\n",
    "    # Check that there is only 1 row from previous generations\n",
    "    # If this is working, then all duplicates are removed as they occur,\n",
    "    # so there shouldn't be any left overs\n",
    "    earliest_acq_row = df_tmp.iloc[0]\n",
    "    generations_acquired = df_tmp[\"gen_acquired\"].tolist()\n",
    "    if len(list(set(generations_acquired))) == 1:\n",
    "        print(\"All duplicates acquired at the same gen | OK\")\n",
    "    else:\n",
    "        mess = \"There shouldn't be more than one duplicate from previous generations\"\n",
    "        num_early_gens = generations_acquired.count(earliest_acq_row[\"gen_acquired\"])\n",
    "        # assert num_early_gens == 1, mess\n",
    "\n",
    "\n",
    "    # Are there multiple early gen rows to choose from?\n",
    "    # Should only happen if multiple are acquired at once\n",
    "    multiple_early_gens_present = False\n",
    "    if len(list(set(generations_acquired))) == 1:\n",
    "        print(\"multiple_early_gens_present\")\n",
    "        multiple_early_gens_present = True\n",
    "\n",
    "#         break\n",
    "\n",
    "    selected_row = \\\n",
    "        df_tmp[df_tmp[\"gen_acquired\"] == earlist_gen].sort_values(\"y_real\").iloc[0]\n",
    "\n",
    "    # lowest_y_row = df_tmp.sort_values(\"y_real\").iloc[0]\n",
    "    # TEMP\n",
    "    # lowest_y_row = df_tmp.sort_values(\"y_real\").iloc[1]\n",
    "    # if earliest_acq_row.name != lowest_y_row.name:\n",
    "    #     print(earliest_acq_row.name, lowest_y_row.name)\n",
    "    # #     model.loc[lowest_y_row.name]\n",
    "    # #     model.rename(\n",
    "    # #         index={\n",
    "    # #             lowest_y_row.name: earliest_acq_row.name + \"_TEMP\",\n",
    "    # #             earliest_acq_row.name: lowest_y_row.name,\n",
    "    # #             }, inplace=True)\n",
    "    # #     model.rename(\n",
    "    # #         index={\n",
    "    # #             earliest_acq_row.name + \"_TEMP\": earliest_acq_row.name,\n",
    "    # #             }, inplace=True)\n",
    "\n",
    "    \n",
    "    indices_that_are_duplicates_i = df_tmp.index.tolist()\n",
    "    indices_that_are_duplicates_i.remove(selected_row.name)\n",
    "\n",
    "    indices_that_are_duplicates.extend(indices_that_are_duplicates_i)\n",
    "    \n",
    "indices_that_are_duplicates = list(set(indices_that_are_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in all_ids_from_duplicate_analysis if i not in indices_that_are_duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp.index.tolist()\n",
    "# print(df_tmp.index.tolist())\n",
    "import copy\n",
    "ids_to_drop = copy.deepcopy(ids_of_duplicates)\n",
    "ids_to_drop.remove(earliest_acq_row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lowest_y_row.name)\n",
    "\n",
    "print(earliest_acq_row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_drop = model.index.intersection(ids_to_drop).tolist()\n",
    "# ids_of_duplicates\n",
    "\n",
    "model.drop(labels=ids_to_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest_y_row.name\n",
    "# earliest_acq_row.name\n",
    "\n",
    "# model.loc['xhbabrx4zq']\n",
    "model.loc['zszinjv3zf']\n",
    "\n",
    "# 'zszinjv3zf' in \n",
    "# model.index.tolist()\n",
    "\n",
    "# model.drop?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_list = []\n",
    "# new_acquisition_dict = {}\n",
    "# for gen_i, AL_i in AL.al_gen_dict.items():\n",
    "#     index_acq_gen_dict_i = dict()\n",
    "#     for index_j in AL_i.new_acquisition:\n",
    "#         index_list.append(index_j)\n",
    "#         index_acq_gen_dict_i[index_j] = int(gen_i)\n",
    "\n",
    "#     new_acquisition_dict.update(index_acq_gen_dict_i)\n",
    "\n",
    "# mess = \"Seems like an id was acquired in more than 1 generation?\"\n",
    "# assert len(index_list) == len(set(index_list)), mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path,\n",
    "    ids_to_discard__too_many_atoms_path,\n",
    "    unique_ids_path,\n",
    "    df_dij_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling\"))\n",
    "from ml_methods import get_data_for_al, get_ml_dataframes\n",
    "\n",
    "out_dict = get_data_for_al(\n",
    "    stoich=\"AB2\",\n",
    "    verbose=False,\n",
    "    drop_too_many_atoms=True)\n",
    "\n",
    "print(out_dict.keys())\n",
    "\n",
    "df_features_pre = out_dict[\"df_features_pre\"]\n",
    "df_static_irox = out_dict[\"df_static_irox\"]\n",
    "df_bulk_dft = out_dict[\"df_bulk_dft\"]\n",
    "df_dij = out_dict[\"df_dij\"]\n",
    "\n",
    "all_indices = df_static_irox.index\n",
    "\n",
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"python_classes\"))\n",
    "from ccf_similarity.ccf import CCF\n",
    "\n",
    "d_thresh = 0.02\n",
    "CCF = CCF(\n",
    "    df_dij=df_dij,\n",
    "    d_thresh=d_thresh,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCF.df_dij.loc[\"v5ckmsnqxi\"]\n",
    "# CCF.df_dij.loc[\"z3ngxhbrz4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_i = \"mkbrzh8kv5\"\n",
    "\n",
    "filter_ids = all_indices\n",
    "\n",
    "simil_dict_master = dict()\n",
    "for index_i in all_indices:\n",
    "    simil_dict = CCF.i_all_similar(index_i, filter_ids=filter_ids)\n",
    "    simil_dict_master[index_i] = simil_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# v5ckmsnqxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CCF.df_dij.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tmp = [i for i in all_indices if i in df_dij.index.tolist()]\n",
    "# len(tmp)\n",
    "# len(all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# self = CCF\n",
    "# d_thresh = self.d_thresh\n",
    "# df_dij = self.df_dij\n",
    "\n",
    "# index_i = \"mkbrzh8kv5\"\n",
    "# # index_j = \"folatese_05\"\n",
    "\n",
    "# # filter_ids = all_indices\n",
    "# filter_ids = None\n",
    "# # #####################################################################\n",
    "\n",
    "\n",
    "# if filter_ids is not None:\n",
    "#     filter_ids_inter = df_dij.index.intersection(filter_ids)\n",
    "#     df_dij = df_dij.loc[filter_ids_inter, filter_ids_inter]\n",
    "\n",
    "# row_i = df_dij.loc[index_i]\n",
    "# row_i = row_i.drop(labels=index_i)\n",
    "\n",
    "# out_dict = row_i[row_i < d_thresh].to_dict()\n",
    "\n",
    "# out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# self = CCF\n",
    "# d_thresh = self.d_thresh\n",
    "# df_dij = self.df_dij\n",
    "\n",
    "\n",
    "# index_i = \"budabebu_36\"\n",
    "# index_j = \"folatese_05\"\n",
    "\n",
    "# dij = df_dij.loc[index_i, index_j]\n",
    "\n",
    "# similar = False\n",
    "# if dij < d_thresh:\n",
    "#     similar = True\n",
    "# elif dij > d_thresh:\n",
    "#     similar = False\n",
    "# else:\n",
    "#     assert False, \"AHHHHHHHH!!!\"\n",
    "\n",
    "# # return(similar)\n",
    "\n",
    "# CCF.i_j_similar()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_static_irox[\"static_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_dij = out_dict[\"df_dij\"]\n",
    "\n",
    "\n",
    "ids_static = df_dij.index.intersection(df_static_irox[\"static_id\"])\n",
    "ids_completed_post_dft = df_dij.index.intersection(df_features_pre.index)\n",
    "\n",
    "ids_dij = ids_static.tolist() + ids_completed_post_dft.tolist()\n",
    "\n",
    "df_dij = df_dij.loc[ids_dij, ids_dij]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# \"bsv4nex29l\" in df_dij.index\n",
    "\n",
    "# for index in df_features_pre.index.tolist():\n",
    "#     if index not in df_dij.index.tolist():\n",
    "#         print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_static_irox.head()\n",
    "# df_static_irox = df_static_irox.loc[\n",
    "#     df_static_irox.index.intersection(\n",
    "#         df_features_pre.index\n",
    "#         ).unique()\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "out_dict = get_ml_dataframes(\n",
    "    names=[\n",
    "        # \"bulk_dft_data_path\",\n",
    "        # \"unique_ids_path\",\n",
    "        # \"prototypes_data_path\",\n",
    "        \"static_irox_structures_path\",\n",
    "        # \"static_irox_structures_kirsten_path\",\n",
    "        # \"oqmd_irox_data_path\",\n",
    "        # \"df_features_pre_opt_path\",\n",
    "        # \"df_features_pre_opt_kirsten_path\",\n",
    "        # \"df_features_post_opt_path\",\n",
    "        # \"df_features_path\",\n",
    "        # \"df_features_cleaned_path\",\n",
    "        # \"df_features_cleaned_pca_path\",\n",
    "        # \"oer_bulk_structures_path\",\n",
    "        # \"df_ccf_path\",\n",
    "        \"df_dij_path\",\n",
    "        # \"ids_to_discard__too_many_atoms_path\",\n",
    "        ],\n",
    "\n",
    "    )\n",
    "\n",
    "out_dict.keys()\n",
    "\n",
    "\n",
    "df_static_irox = out_dict[\"static_irox_structures\"]\n",
    "df_static_irox[\n",
    "    (df_static_irox[\"stoich\"] == stoich) & \\\n",
    "    (df_static_irox[\"source\"] == \"chris\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_dij_path_tmp = df_dij_path[0:-18] + \"df_d_ij_all_temp.pickle\"\n",
    "with open(df_dij_path_tmp, \"rb\") as fle:\n",
    "    df_dij_dft = pickle.load(fle)\n",
    "    print(\"df_dij_dft.shape:\", df_dij_dft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rows_equal_cols = all(df_dij_dft.index == df_dij_dft.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out the issue with the GP (giving error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "import pickle; import os\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"HOME\"],\n",
    "    \"__temp__\",\n",
    "    \"TEMP.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    data = pickle.load(fle)\n",
    "# #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data.keys()\n",
    "\n",
    "K = data[\"K\"]\n",
    "kernel_list = data[\"kernel_list\"]\n",
    "train_matrix = data[\"train_matrix\"]\n",
    "theta = data[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kernel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import cho_solve, cho_factor\n",
    "L, lower = cho_factor(K, overwrite_a=False, lower=True, check_finite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research-new]",
   "language": "python",
   "name": "conda-env-research-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
