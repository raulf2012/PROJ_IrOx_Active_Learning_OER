{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from ccf_similarity.ccf import CCF\n",
    "\n",
    "from active_learning.al_analysis import ALPerformance\n",
    "\n",
    "from plotting.my_plotly import my_plotly_plot\n",
    "\n",
    "# from layout import layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inputs import stoich_i\n",
    "\n",
    "stoich_i = \"AB3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/00_ml_workflow/191102_new_workflow\"))\n",
    "from al_data import al_data_files_dict\n",
    "\n",
    "files_list_gp_ucb = al_data_files_dict[stoich_i][\"files_list_gp_ucb\"]\n",
    "files_list_random = al_data_files_dict[stoich_i][\"files_list_random\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "perc_of_structs = 2.5\n",
    "\n",
    "num_disc = 7\n",
    "\n",
    "subdirs_list = [\"gp_ucb\", \"random\"]\n",
    "\n",
    "shared_scatter_props = dict(\n",
    "    mode=\"lines\",\n",
    "\n",
    "    )\n",
    "\n",
    "data_path_root = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/00_ml_workflow\",\n",
    "    \"191102_new_workflow/00_abx_al_runs/out_data\",\n",
    "    stoich_i,\n",
    "    # \"gp_ucb_False\",\n",
    "    )\n",
    "\n",
    "\n",
    "# from inputs import top_ids_to_track_ab2, top_ids_to_track_ab3\n",
    "\n",
    "# plot_guidlines = False\n",
    "\n",
    "# if stoich_i == \"AB2\":\n",
    "#     top_ids_to_track = top_ids_to_track_ab2\n",
    "# elif stoich_i == \"AB3\":\n",
    "#     top_ids_to_track = top_ids_to_track_ab3\n",
    "# else:\n",
    "#     print(\"ISDJIFSDJI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir=None\n",
    "shared_scatter_props=None\n",
    "unique_scatter_props=None\n",
    "ALPerf_account_duplicates=True\n",
    "top_ids_to_track=None\n",
    "files_list=files_list_gp_ucb\n",
    "color2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# def process_data(\n",
    "# subdir=None,\n",
    "# shared_scatter_props=None,\n",
    "# unique_scatter_props=None,\n",
    "# ALPerf_account_duplicates=True,\n",
    "# top_ids_to_track=None,\n",
    "# files_list=None,\n",
    "# color2=None,\n",
    "# ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "out_data_dict = dict()\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "data_dict = dict()\n",
    "for file_i in files_list:\n",
    "    # #########################################################################\n",
    "    num = file_i.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "    # file_path_i =os.path.join(\n",
    "    #     data_path_root, subdir, file_i)\n",
    "    # COMBAK\n",
    "    # with open(file_path_i, \"rb\") as fle:\n",
    "    with open(file_i, \"rb\") as fle:\n",
    "        AL_i = pickle.load(fle)\n",
    "\n",
    "    data_dict[num] = AL_i\n",
    "out_data_dict[\"AL_dict\"] = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = AL_i.CandidateSpace.FingerPrints.PCA\n",
    "\n",
    "dir(pca)\n",
    "\n",
    "# pca.explained_variance_ratio_\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# # #############################################################################\n",
    "# df_list = []\n",
    "# for num, AL in data_dict.items():\n",
    "#     ALPerf = ALPerformance(\n",
    "#         ALBulkOpt=AL,\n",
    "#         verbose=False)\n",
    "#     ALPerf.num_sys_discovered(\n",
    "#         # perc_of_structs=perc_of_structs,\n",
    "#         # account_duplicates=ALPerf_account_duplicates,\n",
    "\n",
    "#         mode=\"user_specified\",  # 'perc' or 'num'\n",
    "#         # mode=\"perc\",  # 'perc' or 'num'\n",
    "#         perc_of_structs=perc_of_structs,\n",
    "#         num_structs=None,\n",
    "#         ids_to_track=top_ids_to_track,\n",
    "#         account_duplicates=ALPerf_account_duplicates,\n",
    "\n",
    "#         )\n",
    "\n",
    "#     # #########################################################################\n",
    "#     df = ALPerf.num_sys_discovered_df\n",
    "#     df_list.append(df)\n",
    "\n",
    "\n",
    "# df_m = pd.concat(\n",
    "#     df_list,\n",
    "#     axis=1,\n",
    "#     keys=data_dict.keys(),\n",
    "#     )\n",
    "\n",
    "# # Checking that the x-axis series are all the same\n",
    "# # Necessary if the different runs are to be averaged\n",
    "# x_axis_series_list = []\n",
    "# for i in data_dict.keys():\n",
    "#     x_axis_series = df_m[i][\"num_dft\"].tolist()\n",
    "#     x_axis_series_list.append(x_axis_series)\n",
    "# all_x_axis_the_same = all(x_axis_series_list)\n",
    "# assert all_x_axis_the_same is True, \"ISFIDSIFJISDIfj\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # #############################################################################\n",
    "# # df_m.index = df_m[0, \"num_dft\"].tolist()\n",
    "# df_m.index = x_axis_series\n",
    "\n",
    "\n",
    "# # #############################################################################\n",
    "# for i in data_dict.keys():\n",
    "#     del df_m[i, \"num_dft\"]\n",
    "\n",
    "\n",
    "# # TEMP\n",
    "# # out_data_dict[\"df_perf\"] = df_m\n",
    "# # return(out_data_dict)\n",
    "\n",
    "\n",
    "# columns_list = list(df_m.columns.levels[0])\n",
    "# col = df_m.loc[: , columns_list[0]:columns_list[-1]]\n",
    "\n",
    "# # col = df_m.loc[: , 0:list(df_m.columns.levels[0])[-1]]\n",
    "\n",
    "# y_mean = col.mean(axis=1)\n",
    "# y_std = col.std(axis=1)\n",
    "\n",
    "# df_ave = pd.DataFrame()\n",
    "# df_ave[\"y_mean\"] = y_mean\n",
    "# df_ave[\"y_std\"] = y_std\n",
    "# df_ave.index = df_m.index\n",
    "\n",
    "# # Adding 0 to trace\n",
    "# df_ave.loc[0] = [0, 0]\n",
    "# df_ave = df_ave.sort_index()\n",
    "\n",
    "# dx = df_ave.index.values[-1] - df_ave.index.values[-2]\n",
    "# # last_data_point_ind = df_ave.index.values[-1] + dx\n",
    "# last_data_point_ind = df_ave.index.values[-1] + 50\n",
    "# df_ave.loc[last_data_point_ind] = [10, 0]\n",
    "\n",
    "# # df_ave.loc[260] = [10, 0]\n",
    "# df_ave = df_ave.sort_index()\n",
    "\n",
    "\n",
    "# out_data_dict[\"df_perf\"] = df_m\n",
    "\n",
    "# traces = []\n",
    "# # #############################################################################\n",
    "# trace = go.Scatter(\n",
    "#     x=df_ave.index.tolist(),\n",
    "#     y=df_ave[\"y_mean\"],\n",
    "#     line=dict(\n",
    "#         width=1.,\n",
    "#         ),\n",
    "#     )\n",
    "# trace.update(**shared_scatter_props)\n",
    "# trace.update(**unique_scatter_props)\n",
    "# traces.append(trace)\n",
    "# # #########################################################################\n",
    "# trace = go.Scatter(\n",
    "#     x=df_ave.index.tolist(),\n",
    "#     y=df_ave[\"y_mean\"] + df_ave[\"y_std\"],\n",
    "#     line=dict(\n",
    "#         width=0.5,\n",
    "#         ),\n",
    "#     )\n",
    "# trace.update(**shared_scatter_props)\n",
    "# trace.update(**unique_scatter_props)\n",
    "# traces.append(trace)\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     x=df_ave.index.tolist(),\n",
    "#     y=df_ave[\"y_mean\"] - df_ave[\"y_std\"],\n",
    "#     fill=\"tonexty\",\n",
    "#     line=dict(\n",
    "#         width=0.5,\n",
    "#         # color=\"red\",\n",
    "#         ),\n",
    "#     )\n",
    "# trace.update(**shared_scatter_props)\n",
    "# trace.update(**unique_scatter_props)\n",
    "# traces.append(trace)\n",
    "# # #########################################################################\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     x=df_ave.index.tolist(),\n",
    "#     y=df_ave[\"y_mean\"] - df_ave[\"y_std\"],\n",
    "#     line=dict(\n",
    "#         width=0.5,\n",
    "#         color=color2,\n",
    "#         ),\n",
    "#     )\n",
    "# trace.update(**shared_scatter_props)\n",
    "# trace.update(**unique_scatter_props)\n",
    "# traces.append(trace)\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     x=df_ave.index.tolist(),\n",
    "#     y=df_ave[\"y_mean\"] + df_ave[\"y_std\"],\n",
    "#     line=dict(\n",
    "#         width=0.5,\n",
    "#         color=color2,\n",
    "#         ),\n",
    "#     )\n",
    "# trace.update(**shared_scatter_props)\n",
    "# trace.update(**unique_scatter_props)\n",
    "# traces.append(trace)\n",
    "\n",
    "# # out_data_dict[\"trace\"] = trace\n",
    "# out_data_dict[\"trace\"] = traces\n",
    "# out_data_dict[\"df_ave\"] = df_ave"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
