{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New ML Active Learning Workflow\n",
    "---\n",
    "\n",
    "A model that predicts the mean (~ -6.05 eV/atom) has a MAE of ~0.3 eV/atom)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# | - OUT_OF_SIGHT\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.environ[\"PROJ_irox\"], \"data\"))\n",
    "from proj_data_irox import (\n",
    "    bulk_dft_data_path, unique_ids_path,\n",
    "    df_features_pre_opt_path,\n",
    "    df_features_post_opt_path)\n",
    "\n",
    "\n",
    "from plotting.my_plotly import my_plotly_plot\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "\n",
    "sys.path.insert(0,\n",
    "    os.path.join(os.environ[\"PROJ_irox\"], \"workflow/ml_modelling\"))\n",
    "\n",
    "sys.path.insert(0, os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/00_ml_workflow/190611_new_workflow/02_gaus_proc\"))\n",
    "\n",
    "from gp_methods import gp_workflow, job_aquisition, test_al_conv\n",
    "from methods import get_trace_j\n",
    "from gp_methods import gp_model_gpflow, gp_model_catlearn\n",
    "from ml_methods import create_mixed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# stoich_i = \"AB2\"\n",
    "stoich_i = \"AB3\"\n",
    "\n",
    "# gp_model = gp_model_gpflow\n",
    "gp_model = gp_model_catlearn\n",
    "\n",
    "aqs_bin_size = 5\n",
    "\n",
    "# output_key = \"form_e_chris\"\n",
    "output_key = \"energy_pa\"\n",
    "\n",
    "verbosity_level = 6  # 1-10 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "#     \"noise\": [0.02542],\n",
    "#     \"sigma_l\": [0.0049],\n",
    "#     \"sigma_f\": [5.19],\n",
    "#     \"alpha\": [0.018],\n",
    "\n",
    "    \"noise\": [0.0001],\n",
    "    \"sigma_l\": [10.],\n",
    "    \"sigma_f\": [5],\n",
    "    \"alpha\": [0.1],\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "c = list(itertools.product(*params_dict.values()))\n",
    "df_gp_params = pd.DataFrame(c, columns=params_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {},
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "with open(bulk_dft_data_path, \"rb\") as fle:\n",
    "    df_bulk_dft = pickle.load(fle)\n",
    "\n",
    "with open(df_features_pre_opt_path, \"rb\") as fle:\n",
    "    df_features_pre = pickle.load(fle)\n",
    "\n",
    "with open(df_features_post_opt_path, \"rb\") as fle:\n",
    "    df_features_post = pickle.load(fle)\n",
    "\n",
    "df_ids = pd.read_csv(unique_ids_path)\n",
    "\n",
    "print(\"df_ids.shape:\", df_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# iro2_indices_already_computed = df_bulk_dft[df_bulk_dft[\"stoich\"] == \"AB2\"].index\n",
    "\n",
    "\n",
    "# df_ids_tmp = df_ids[df_ids[\"stoich\"] == \"AB2\"].set_index(\"unique_ids\")\n",
    "# df_ids_tmp.drop(iro2_indices_already_computed)[0:50][\"id\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems to Discard"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/ccf_similarity_analysis/out_data\",\n",
    "    \"all_ids_to_elim.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    ids_to_drop = pickle.load(fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/ccf_similarity_analysis/out_data\",\n",
    "    \"all_ids_to_elim.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    ids_to_drop__duplicates = pickle.load(fle)\n",
    "    \n",
    "# #############################################################################\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox\"],\n",
    "    \"workflow/ml_modelling/visualizing_data/out_data\",\n",
    "    \"outlier_features.json\")\n",
    "with open(path_i, 'r') as f:\n",
    "    ids_to_drop__outliers = json.load(f)\n",
    "\n",
    "# #############################################################################\n",
    "ids_to_drop = ids_to_drop__outliers + ids_to_drop__duplicates\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))\n",
    "ids_to_drop = list(set(ids_to_drop))\n",
    "print(\"len(ids_to_drop):\", len(ids_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering dataframes to the correct stoicheometry"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Filter ids ##################################################################\n",
    "print(\"df_ids.shape:\", df_ids.shape)\n",
    "df_ids = df_ids[\n",
    "    (df_ids[\"stoich\"] == stoich_i) & \\\n",
    "    (df_ids[\"source\"] != \"oqmd\") & \\\n",
    "    # (df_ids[\"source\"] != \"raul\")\n",
    "    [True for i in range(len(df_ids))]\n",
    "    ]\n",
    "\n",
    "print(\"df_ids.shape:\", df_ids.shape)\n",
    "\n",
    "# IDS TO DROP\n",
    "df_ids = df_ids[~df_ids[\"unique_ids\"].isin(ids_to_drop)]\n",
    "\n",
    "print(\"df_ids.shape:\", df_ids.shape)\n",
    "\n",
    "unique_ids = df_ids[\"unique_ids\"].tolist()\n",
    "\n",
    "# #############################################################################\n",
    "# Training Features ###########################################################\n",
    "index_filter = np.intersect1d(df_features_post.index, unique_ids)\n",
    "df_features_post = df_features_post.loc[index_filter]\n",
    "\n",
    "# #############################################################################\n",
    "# Training Features ###########################################################\n",
    "index_filter = np.intersect1d(df_bulk_dft.index, unique_ids)\n",
    "df_bulk_dft = df_bulk_dft.loc[index_filter]\n",
    "print(\"df_bulk_dft.shape:\", df_bulk_dft.shape)\n",
    "\n",
    "# #############################################################################\n",
    "# Test Features ###############################################################\n",
    "index_filter = np.intersect1d(df_features_pre.index, unique_ids)\n",
    "df_features_pre = df_features_pre.loc[index_filter]\n",
    "\n",
    "# #############################################################################\n",
    "# Filter training data ########################################################\n",
    "# df_features_post = \\\n",
    "#     df_features_post[df_features_post[\"data\"][\"source\"] != \"chris\"]\n",
    "# df_bulk_dft = df_bulk_dft[df_bulk_dft[\"source\"] != \"chris\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sys.path.insert(0,\n",
    "    os.path.join(os.environ[\"PROJ_irox\"], \"workflow/ml_modelling\"))\n",
    "\n",
    "all_ids = df_features_pre.index.unique()\n",
    "\n",
    "computed_ids = df_bulk_dft.index.unique()\n",
    "computed_ids = np.random.choice(computed_ids, size=10)\n",
    "computed_ids = list(computed_ids)\n",
    "\n",
    "df_post = df_features_post[\"voronoi\"]\n",
    "df_pre = df_features_pre[\"voronoi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing N-fold CV Folds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_bulk_dft:\", df_bulk_dft.shape)\n",
    "\n",
    "n_fold_cv = df_bulk_dft.shape[0]\n",
    "n_fold_cv = 5\n",
    "\n",
    "\n",
    "fold_size = int(df_bulk_dft.shape[0] / n_fold_cv)\n",
    "print(\"fold_size:\", fold_size)\n",
    "\n",
    "# Shuffling training data\n",
    "df_bulk_dft = df_bulk_dft.sample(\n",
    "    n=None,\n",
    "    frac=1.,\n",
    "    replace=False,\n",
    "    axis=None)\n",
    "\n",
    "print(\"n_fold_cv * fold_size:\", n_fold_cv * fold_size)\n",
    "\n",
    "ids_0 = df_bulk_dft.index[:n_fold_cv * fold_size]\n",
    "folds = np.split(ids_0, n_fold_cv)\n",
    "\n",
    "ids_leftover = df_bulk_dft.index[n_fold_cv * fold_size:]\n",
    "\n",
    "if ids_leftover.shape[0] > 0:\n",
    "    folds.append(ids_leftover)\n",
    "\n",
    "folds = np.array(folds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "models_inst_list = []\n",
    "out_list = []\n",
    "# for i_cnt, (name_i, row_i) in enumerate(df_bulk_dft.iterrows()):\n",
    "for i_cnt, fold_i in enumerate(folds):\n",
    "    # | - GP AL Iteration ******************************************************\n",
    "    # *************************************************************************\n",
    "    # *************************************************************************\n",
    "    print(\"\")\n",
    "    t0 = time.time()\n",
    "    num_training = str(len(fold_i)).zfill(3)\n",
    "    step_num = str(i_cnt).zfill(3);\n",
    "    print(step_num, \" | \", num_training + \" \" + 68 * \"#\"); print(80 * \"#\")\n",
    "    row_i = df_gp_params.iloc[0]\n",
    "\n",
    "\n",
    "    df_bulk_dft_i = df_bulk_dft.drop(\n",
    "        # labels=name_i,\n",
    "        labels=fold_i,\n",
    "        axis=0)\n",
    "\n",
    "    df_train = df_post.loc[df_bulk_dft_i.index]\n",
    "    # df_test_tmp = df_post.loc[[name_i]]\n",
    "    df_test_tmp = df_post.loc[fold_i]\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # Running GP Model ########################################################\n",
    "    if True:\n",
    "#     try:\n",
    "        gp_params_i = row_i.to_dict()\n",
    "        out = gp_workflow(\n",
    "            df_features_post=df_train,\n",
    "            df_test=df_test_tmp,\n",
    "            df_bulk_dft=df_bulk_dft_i,\n",
    "            df_bulk_dft_all=df_bulk_dft,\n",
    "\n",
    "            df_ids=df_ids,\n",
    "            gp_model=gp_model_catlearn,\n",
    "            opt_hyperparameters=True,\n",
    "            gp_params=gp_params_i,\n",
    "            y_train_key=\"energy_pa\",\n",
    "\n",
    "            verbose=False,\n",
    "\n",
    "            clean_variance_flag=True,\n",
    "            clean_skewness_flag=True,\n",
    "            clean_infinite_flag=True,\n",
    "            standardize_data_flag=True,\n",
    "\n",
    "\n",
    "            pca_comp=11,\n",
    "            # pca_comp=11,\n",
    "            pca_perc=0.99,\n",
    "            pca_mode=\"num_comp\",\n",
    "            # pca_mode=\"perc\",\n",
    "            ); out_list.append(out)\n",
    "\n",
    "        model_i = out[\"model\"]; model_inst = out[\"model_inst\"]\n",
    "\n",
    "\n",
    "        models_inst_list.append(model_inst)\n",
    "        test_row_i = model_i[model_i[\"prediction\"].notnull()]\n",
    "        rows_list.append(test_row_i)\n",
    "\n",
    "\n",
    "        mae_i = abs(test_row_i[\"prediction_unstandardized\"] - test_row_i[\"energy_pa\"]).mean()\n",
    "        print(\n",
    "            \"MAE_i: \",\n",
    "            mae_i)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"model_inst.regularization\", model_inst.regularization)\n",
    "        \n",
    "        print(\"model_inst.kernel_list:\", model_inst.kernel_list)\n",
    "\n",
    "#         try:\n",
    "#             print(\"kernel_list['slope']:\", model_inst.kernel_list[0][\"slope\"])\n",
    "#             print(\"kernel_list['scaling']\", model_inst.kernel_list[0][\"scaling\"])\n",
    "#             print(\"kernel_list['degree']\", model_inst.kernel_list[0][\"degree\"][0])\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         try:\n",
    "#             print(\"width:\", model_inst.kernel_list[0][\"width\"])\n",
    "#             print(\"scaling\", model_inst.kernel_list[0][\"scaling\"])\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "        print(\n",
    "            \"model_inst.log_marginal_likelihood: \",\n",
    "            model_inst.log_marginal_likelihood)\n",
    "\n",
    "#     except:\n",
    "#         print(\"Failed!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing PCA"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = out[\"pca\"]\n",
    "\n",
    "columns_pre_pca = out[\"df_train_pre_pca\"].columns\n",
    "\n",
    "df_pca_comp = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=columns_pre_pca,\n",
    "    # index = ['PC-1','PC-2']\n",
    "    index=[\"PCA_\" + str(i).zfill(2) for i in range(pca.n_components)],\n",
    "    )\n",
    "\n",
    "df_pca_comp = abs(df_pca_comp.T)\n",
    "\n",
    "# df_pca_comp\n",
    "# df_pca_comp.sort_values(\"PCA_00\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc MAE"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pred = pd.concat(rows_list)\n",
    "\n",
    "df_all_pred[\"error\"] = abs(\n",
    "    df_all_pred[\"prediction_unstandardized\"] - df_all_pred[\"energy_pa\"])\n",
    "\n",
    "mae = df_all_pred[\"error\"].mean()\n",
    "\n",
    "print(\"MAE (eV): \", mae)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MAE (eV):  0.14186230367245345\n",
    "\n",
    "MAE (eV):  0.1352335205391316\n",
    "\n",
    "MAE (eV):  0.12055492648019178"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pred.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Without any optimization\n",
    "MAE (eV):  0.08421070878633005\n",
    "\n",
    "# With only local opt\n",
    "MAE (eV):  0.08245272125264294\n",
    "\n",
    "# With Global opt (a lot failed)\n",
    "MAE (eV):  0.07163131081485057"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Removing duplicates via CCF analysis (3 PCA components)\n",
    "MAE (eV):  0.1946592371070702\n",
    "\n",
    "\n",
    "# Removing duplicates via CCF analysis (99% variance captured w/ PCA)\n",
    "MAE (eV):  0.1341779368327114"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 PCA Components | Optmization"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizatino on\n",
    "0.13447096075807743\n",
    "\n",
    "# Global Optimization on\n",
    "0.13244164782822093\n",
    "\n",
    "# No optimization\n",
    "0.13184483308309516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_pca_dict = {\n",
    "    30: 0.1831923511185789,\n",
    "    20: 0.14531282803281226,\n",
    "    15: 0.13394643100712159,\n",
    "\n",
    "    13: 0.12004683884649776,\n",
    "    12: 0.11351107652779499,\n",
    "    11: 0.11091405471923632,\n",
    "\n",
    "    # 10: 0.13401349790117334,    \n",
    "    10: 0.1298666071766859,\n",
    "\n",
    "    9: 0.13246529211025734,\n",
    "    8: 0.13562396520300607,\n",
    "    7: 0.13272655356364796,\n",
    "    6: 0.1333824520990277,\n",
    "    5: 0.13447096075807743,\n",
    "    4: 0.1384055763748185,\n",
    "    3: 0.15767416200944745,\n",
    "    2: 0.22931482558271382,\n",
    "    1: 0.21253287970745607,\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(mae_pca_dict, index=[\"mae\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import os\n",
    "\n",
    "y_array = df[\"mae\"]\n",
    "x_array = df.index.tolist()\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=x_array,\n",
    "    y=y_array,\n",
    "    mode=\"markers\",\n",
    "\n",
    "    marker=dict(\n",
    "        symbol=\"circle\",\n",
    "        color='LightSkyBlue',\n",
    "#         colorscale='Viridis',\n",
    "        colorbar=dict(thickness=20),\n",
    "        size=20,\n",
    "        line=dict(\n",
    "            color='MediumPurple',\n",
    "            width=2\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
