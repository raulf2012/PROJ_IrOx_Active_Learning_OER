{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| - Import Modules\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from ase import io\n",
    "\n",
    "import shutil\n",
    "\n",
    "from methods import parse_job_err\n",
    "from methods import parse_finished_file\n",
    "from methods import parse_job_state\n",
    "from methods import is_job_submitted\n",
    "from methods import get_isif_from_incar\n",
    "from methods import read_write_CONTCAR\n",
    "from methods import get_number_of_ionic_steps\n",
    "\n",
    "\n",
    "from methods import set_up__submit__new_job\n",
    "#__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from inputs import input_dict\n",
    "#     root_dir = input_dict[\"root_dir\"]\n",
    "    root_dir = input_dict.get(\"root_dir\", \".\")\n",
    "    ignore_ids = input_dict.get(\"ignore_ids\", [])\n",
    "except:\n",
    "    print(\"Couldn't import inputs script\")\n",
    "    input_dict = {}\n",
    "    root_dir = \".\"\n",
    "    ignore_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# Set to True when you're actually ready\n",
    "change_file_sys = False\n",
    "\n",
    "# root_dir = \"__test__/job_folders_2\"\n",
    "# root_dir = \".\"\n",
    "# root_dir = \"__test__/iro2_calcs_test\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Job rules\n",
    "\n",
    "## If the job is running or pending or configuring, ignore for now\n",
    "## If the job timed_out or has a filed job_state, then resubmit\n",
    "## If the job has succeeded job_state, then check if isif is 7 or 3,\n",
    "  ## if 7 resubmit with 3, if 3 then resubmit 1 more time with 3, then finally submit one more time with isif 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing dir to find job folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_pre_paths = []\n",
    "if False:\n",
    "    path_i = \"out_data/df_dict.pickle\"\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        data = pickle.load(fle)\n",
    "        df_tmp = data[\"df\"]\n",
    "        done_pre_paths = df_tmp[\"pre_path\"].tolist()\n",
    "\n",
    "# except:\n",
    "#     print(\"THIS FAILED!!\")\n",
    "#     done_pre_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done_pre_paths:\", done_pre_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "job_dirs = []\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    print(\"TEMP: \", subdir)\n",
    "\n",
    "    if subdir in done_pre_paths:\n",
    "        print(\"skipping this dir, already done\")\n",
    "        continue\n",
    "\n",
    "    last_dir = subdir.split(\"/\")[-1]\n",
    "    cond_0 = last_dir[0] == \"_\"\n",
    "    cond_1 = last_dir[1:].isdigit()\n",
    "    if cond_0 and cond_1:\n",
    "        print(\"Parsing dirs:\", subdir)\n",
    "\n",
    "        revision_i = int(last_dir[1:])\n",
    "\n",
    "        job_pre_path_i = \"/\".join(subdir.split(\"/\")[:-1])\n",
    "        id_i = job_pre_path_i.split(\"/\")[-1]\n",
    "\n",
    "        try:\n",
    "            atoms_path_i = os.path.join(subdir, \"OUTCAR\")\n",
    "            atoms_i = io.read(atoms_path_i)\n",
    "        except:\n",
    "            atoms_i = None\n",
    "\n",
    "        out_dict = dict(\n",
    "            path=subdir,\n",
    "            pre_path=job_pre_path_i,\n",
    "            id=id_i,\n",
    "            revision=revision_i,\n",
    "            atoms=atoms_i)\n",
    "        data_list.append(out_dict)\n",
    "\n",
    "        job_dirs.append(subdir)\n",
    "\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing jobs to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path_i = \"/home/raulf2012/Dropbox/01_norskov/PROJECT_DATA/04_IrOx_surfaces_OER/ml_bulk_irox_dft/iro3/df_dict.pickle\"\n",
    "\n",
    "# try:\n",
    "#     path_i = \"out_data/df_dict.pickle\"\n",
    "#     with open(path_i, \"rb\") as fle:\n",
    "#         data = pickle.load(fle)\n",
    "\n",
    "#     df_new_jobs = data[\"df_new_jobs\"]\n",
    "# #     df_tmp = data[\"df\"]\n",
    "\n",
    "#     done_str = \"ALL DONE! | ISIF 2\"\n",
    "#     completed_jobs_pre_path_list = df_new_jobs[\n",
    "#         df_new_jobs[\"action\"] == done_str][\"pre_path\"].tolist()\n",
    "\n",
    "#     df = df[~df[\"pre_path\"].isin(completed_jobs_pre_path_list)]\n",
    "# except:\n",
    "#     print(\"Couldn't reduce `pre_path` to save time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing dirs to get job state info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| - Parsing dirs to get job state info\n",
    "# #############################################################################\n",
    "print(\"Parsing dirs to get job state info\")\n",
    "print(80 * \"&\")\n",
    "print(80 * \"&\")\n",
    "print(80 * \"&\")\n",
    "print(80 * \"&\")\n",
    "\n",
    "def method(row_i):\n",
    "    status_dict = parse_job_err(row_i[\"path\"])\n",
    "    for key, value in status_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "df = df.apply(method, axis=1)\n",
    "\n",
    "# #############################################################################\n",
    "def method(row_i):\n",
    "    status_dict = parse_finished_file(row_i[\"path\"])\n",
    "    for key, value in status_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "df = df.apply(method, axis=1)\n",
    "\n",
    "# #############################################################################\n",
    "def method(row_i):\n",
    "    status_dict = parse_job_state(row_i[\"path\"])\n",
    "    for key, value in status_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "df = df.apply(method, axis=1)\n",
    "\n",
    "# #############################################################################\n",
    "def method(row_i):\n",
    "    status_dict = is_job_submitted(row_i[\"path\"])\n",
    "    for key, value in status_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "df = df.apply(method, axis=1)\n",
    "\n",
    "# #############################################################################\n",
    "def method(row_i):\n",
    "    status_dict = get_isif_from_incar(row_i[\"path\"])\n",
    "    for key, value in status_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "df = df.apply(method, axis=1)\n",
    "\n",
    "# #############################################################################\n",
    "def method(row_i):\n",
    "    status_dict = get_number_of_ionic_steps(row_i[\"path\"])\n",
    "    for key, value in status_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "df = df.apply(method, axis=1)\n",
    "\n",
    "#__|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with open(os.path.join(directory, \"df.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df, fle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(80 * \"*\"); print(80 * \"*\")\n",
    "\n",
    "# unique_pre_paths = df[\"pre_path\"].unique()\n",
    "\n",
    "data_list = []\n",
    "grouped = df.groupby([\"pre_path\"])\n",
    "for name, group in grouped:\n",
    "#     group = df[df[\"pre_path\"] == \"__test__/job_folders/000\"]\n",
    "\n",
    "    data_dict_i = {}\n",
    "\n",
    "    group = group.sort_values(\"revision\", ascending=False)\n",
    "\n",
    "\n",
    "    # Number of completed isif = 3 jobs\n",
    "    # Should be run maybe 2-3 times to fully converge\n",
    "    df_tmp = group[group[\"isif\"] == 3]\n",
    "    num_completed_isif_3 = len(df_tmp[df_tmp[\"completed\"] == True])\n",
    "\n",
    "    data_dict_i[\"num_completed_isif_3\"] = num_completed_isif_3\n",
    "\n",
    "    latest_rev = group.iloc[0]\n",
    "\n",
    "    data_dict_i[\"num_revisions\"] = latest_rev[\"revision\"]\n",
    "\n",
    "    print(\"Main\", latest_rev[\"path\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #########################################################################\n",
    "    # #########################################################################\n",
    "    # #########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    path = latest_rev[\"path\"]\n",
    "    job_state = latest_rev[\"job_state\"]\n",
    "    timed_out = latest_rev[\"timed_out\"]\n",
    "    isif = latest_rev[\"isif\"]\n",
    "    completed = latest_rev[\"completed\"]\n",
    "    pre_path = latest_rev[\"pre_path\"]\n",
    "    id_i = latest_rev[\"id\"]\n",
    "\n",
    "    failed = latest_rev[\"error\"]\n",
    "    error_type = latest_rev[\"error_type\"]\n",
    "\n",
    "    data_dict_i[\"pre_path\"] = pre_path\n",
    "    data_dict_i[\"id\"] = id_i\n",
    "\n",
    "    skip_job = False\n",
    "\n",
    "    new_job_file_dict = dict()\n",
    "\n",
    "    # Ignoring manually selected jobs\n",
    "    if id_i in ignore_ids:\n",
    "        data_dict_i[\"action\"] = \"Ignoring this id\"\n",
    "        skip_job = True\n",
    "\n",
    "    else:\n",
    "\n",
    "        cond_0 = job_state == \"RUNNING\"\n",
    "        cond_1 = job_state == \"PENDING\"\n",
    "        cond_2 = job_state == \"CONFIGURING\"\n",
    "        if cond_0 or cond_1 or cond_2:\n",
    "            #| - If job is either running, pending or being configured\n",
    "            # (whatever that means), just move on for now\n",
    "            mess_i = \"Job is busy, will skip\"\n",
    "            data_dict_i[\"action\"] = mess_i\n",
    "\n",
    "            skip_job = True\n",
    "            pass\n",
    "            #__|\n",
    "\n",
    "        elif job_state == \"SUCCEEDED\" or completed:\n",
    "            #| - SUCCEEDED\n",
    "            # Picking the model.py script to use\n",
    "            mess_i = \"Job done\"\n",
    "            data_dict_i[\"action\"] = mess_i\n",
    "\n",
    "            read_write_CONTCAR(path, new_job_file_dict)\n",
    "\n",
    "            if isif == 7:\n",
    "                model_file_path = os.path.join(\n",
    "                    os.environ[\"PROJ_irox\"],\n",
    "                    \"run_nersc_vasp/ml_bulk_opt\",\n",
    "                    \"bulk_opt_last.py\")\n",
    "                new_job_file_dict[model_file_path] = \"model.py\"\n",
    "\n",
    "                data_dict_i[\"action\"] += \" | ISIF 7 done, --> ISIF 3\"\n",
    "\n",
    "            elif isif == 3:\n",
    "                data_dict_i[\"action\"] += \" | ISIF 3 done\"\n",
    "\n",
    "                if num_completed_isif_3 < 3:\n",
    "                    model_file_path = os.path.join(\n",
    "                        os.environ[\"PROJ_irox\"],\n",
    "                        \"run_nersc_vasp/ml_bulk_opt\",\n",
    "                        \"bulk_opt_last.py\")\n",
    "                    new_job_file_dict[model_file_path] = \"model.py\"\n",
    "                    data_dict_i[\"action\"] += \" | Rerunning isif 3\"\n",
    "                else:\n",
    "                    model_file_path = os.path.join(\n",
    "                        os.environ[\"PROJ_irox\"],\n",
    "                        \"run_nersc_vasp/ml_bulk_opt\",\n",
    "                        \"bulk_opt_init_final_isif_2.py\")\n",
    "                    new_job_file_dict[model_file_path] = \"model.py\"\n",
    "                    data_dict_i[\"action\"] += \" | --> isif 2\"\n",
    "\n",
    "\n",
    "            elif isif == 2:\n",
    "                skip_job = True\n",
    "                data_dict_i[\"action\"] = \"ALL DONE! | ISIF 2\"\n",
    "\n",
    "                pass\n",
    "            #__|\n",
    "\n",
    "        elif timed_out or failed:\n",
    "            #| - Timed out of failed\n",
    "            print(\"timed out or failed\")\n",
    "\n",
    "            if error_type == \"Error in SGRCON (symm error)\":\n",
    "                data_dict_i[\"action\"] = \"Error, need manual attention\"\n",
    "                skip_job = True\n",
    "            else:\n",
    "                data_dict_i[\"action\"] = \"Time out or failed\"\n",
    "\n",
    "                read_write_CONTCAR(path, new_job_file_dict)\n",
    "\n",
    "\n",
    "                # Picking the model.py script to use\n",
    "                if isif == 7:\n",
    "                    model_file_path = os.path.join(\n",
    "                        os.environ[\"PROJ_irox\"],\n",
    "                        \"run_nersc_vasp/ml_bulk_opt\",\n",
    "                        \"bulk_opt_init.py\")\n",
    "                    new_job_file_dict[model_file_path] = \"model.py\"\n",
    "                    data_dict_i[\"action\"] += \" | Restarting isif 7 calc\"\n",
    "\n",
    "                elif isif == 3:\n",
    "                    model_file_path = os.path.join(\n",
    "                        os.environ[\"PROJ_irox\"],\n",
    "                        \"run_nersc_vasp/ml_bulk_opt\",\n",
    "                        \"bulk_opt_last.py\")\n",
    "                    new_job_file_dict[model_file_path] = \"model.py\"\n",
    "                    data_dict_i[\"action\"] += \" | Restarting isif 3 calc\"\n",
    "\n",
    "                elif isif == 2:\n",
    "                    model_file_path = os.path.join(\n",
    "                        os.environ[\"PROJ_irox\"],\n",
    "                        \"run_nersc_vasp/ml_bulk_opt\",\n",
    "                        \"bulk_opt_init_final_isif_2.py\")\n",
    "                    new_job_file_dict[model_file_path] = \"model.py\"\n",
    "                    data_dict_i[\"action\"] += \" | Restarting isif 2 calc\"\n",
    "            #__|\n",
    "\n",
    "        else:\n",
    "            skip_job = True\n",
    "\n",
    "            mess_i = \"Couldn't figure out what to do\"\n",
    "            data_dict_i[\"action\"] = mess_i\n",
    "            print(mess_i)\n",
    "\n",
    "\n",
    "    if not skip_job and change_file_sys:\n",
    "        set_up__submit__new_job(\n",
    "            latest_rev,\n",
    "            new_job_file_dict,\n",
    "            run_calc=True)\n",
    "\n",
    "    data_list.append(data_dict_i)\n",
    "\n",
    "df_new_jobs = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_done = df_new_jobs[df_new_jobs[\"action\"] != \"ALL DONE! | ISIF 2\"]\n",
    "\n",
    "df_timed_out = df_new_jobs[df_new_jobs[\"action\"] == \"Time out or failed | Restarting isif 3 calc\"]\n",
    "\n",
    "df_not_sure = df_new_jobs[df_new_jobs[\"action\"] == \"Couldn't figure out what to do\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(80 * \"#\")\n",
    "print(\"df_not_sure | df_timed_out | df_not_done | df_new_jobs\")\n",
    "print(80 * \"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data and uploading to Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "\n",
    "directory = \"out_data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "df_dict = {\n",
    "    \"df\": df,\n",
    "    \"df_new_jobs\": df_new_jobs,\n",
    "    }\n",
    "\n",
    "import datetime\n",
    "datetime_i = datetime.datetime.now().strftime(\"%y%m%d__%H_%M\")\n",
    "\n",
    "filename_i = \"out_data/\" + \"df_dict.pickle\"\n",
    "with open(filename_i, \"wb\") as fle:\n",
    "    pickle.dump(df_dict, fle)\n",
    "\n",
    "filename_i = \"out_data/\" + datetime_i + \"_df_dict.pickle\"\n",
    "with open(filename_i, \"wb\") as fle:\n",
    "    pickle.dump(df_dict, fle)\n",
    "\n",
    "\n",
    "\n",
    "if os.environ[\"USER\"] == \"flores12\":\n",
    "\n",
    "    db_path_bkp = os.path.join(\n",
    "        \"01_norskov/00_git_repos/PROJ_IrOx_Active_Learning_OER\",\n",
    "        \"run_nersc_vasp/ml_bulk_opt/run_all_bulks/out_data/\")\n",
    "\n",
    "    db_path = input_dict.get(\n",
    "        \"proj_data_save_dir\",\n",
    "        db_path_bkp)\n",
    "\n",
    "    out_file_name = input_dict.get(\n",
    "        \"out_file_name\",\n",
    "        \"df_dict.pickle\")\n",
    "\n",
    "    db_path = os.path.join(db_path, out_file_name)\n",
    "\n",
    "    \n",
    "    # bash_comm = \"rclone copy out_data/df_dict.pickle raul_dropbox:\" + db_path\n",
    "    bash_comm = \"rclone copyto \" + filename_i + \" raul_dropbox:\" + db_path\n",
    "    print(bash_comm)\n",
    "    os.system(bash_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting temp files\n",
    "os.system(\"rm init.cif run_vasp.py\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Coping over job folders to easily recover state\n",
    "\n",
    "# shutil.rmtree(\"./__test__/job_folders\")\n",
    "\n",
    "# shutil.copytree(\n",
    "#     \"./__test__/job_folders.orig\",\n",
    "#     \"./__test__/job_folders\",\n",
    "#     )\n",
    "\n",
    "# df_new_jobs\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from methods import parse_job_err\n",
    "\n",
    "# row_i = df.iloc[0]\n",
    "# path_i = row_i[\"path\"]\n",
    "# path_i\n",
    "\n",
    "\n",
    "\n",
    "# parse_job_err(path_i)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
